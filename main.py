import os
import requests
import sqlite3
from datetime import datetime, timedelta, timezone
import tempfile
import base64
import json
import io

import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from dotenv import load_dotenv
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload

load_dotenv()
LINE_TOKEN = os.environ["LINE_CHANNEL_ACCESS_TOKEN"]
LINE_USER_ID = os.environ["LINE_USER_ID"]

DATA_DIR = os.environ.get("DATA_DIR", "data")
DB_PATH = os.path.join(DATA_DIR, "taiex.sqlite")
GDRIVE_FOLDER_NAME = "stocks-autobot-data"
GDRIVE_DATA_FOLDER = "data"  # åœ¨stocks-autobot-dataä¸‹çš„å­è³‡æ–™å¤¾

# Google Drive Service Account setup
SCOPES = ['https://www.googleapis.com/auth/drive']

# Use environment variable for stock codes, fallback to comprehensive list
DEFAULT_CODES = [
    "2330", "2317", "2454", "2881", "2882", "2886", "2412", "2891", "2303", "1301",
    "1303", "2308", "2002", "2884", "2892", "2912", "2885", "1326", "2395", "2357",
    "3711", "2382", "1216", "2207", "6505", "2603", "2609", "2615", "3008", "2327",
    "2880", "2887", "5880", "2890", "3045", "4904", "5871", "2324", "2301", "4938",
    "2409", "3037", "2344", "2345", "6415", "3034", "2379", "3231", "2408", "3661",
    "3443", "2449", "2377", "6669", "2474", "1102", "2105", "2201", "2227", "6176",
    "9910", "2834", "2801", "2049", "2353", "2354", "2356", "3706", "5269", "6239",
    "6271", "3481", "4961", "2360", "2385", "2458", "3545", "8046", "2368", "2371",
    "2376", "6282", "4968", "3017", "2347", "2393", "6116", "8150", "2313", "2337",
    "6414", "2107", "2204", "2362", "2427", "5876", "9945", "2101", "2364", "3702"
]

CODES = os.environ.get("TWSE_CODES", ",".join(DEFAULT_CODES)).split(",")
PICKS_TOP_K = int(os.environ.get("TOP_K", "100"))

STOCK_NAMES = {
    "2330": "å°ç©é›»", "2317": "é´»æµ·", "2454": "è¯ç™¼ç§‘", "2881": "å¯Œé‚¦é‡‘", "2882": "åœ‹æ³°é‡‘",
    "2886": "å…†è±é‡‘", "2412": "ä¸­è¯é›»", "2891": "ä¸­ä¿¡é‡‘", "2303": "è¯é›»", "1301": "å°å¡‘",
    "1303": "å—äº", "2308": "å°é”é›»", "2002": "ä¸­é‹¼", "2884": "ç‰å±±é‡‘", "2892": "ç¬¬ä¸€é‡‘",
    "2912": "çµ±ä¸€è¶…", "2885": "å…ƒå¤§é‡‘", "1326": "å°åŒ–", "2395": "ç ”è¯", "2357": "è¯ç¢©",
    "3711": "æ—¥æœˆå…‰æŠ•æ§", "2382": "å»£é”", "1216": "çµ±ä¸€", "2207": "å’Œæ³°è»Š", "6505": "å°å¡‘åŒ–",
    "2603": "é•·æ¦®", "2609": "é™½æ˜", "2615": "è¬æµ·", "3008": "å¤§ç«‹å…‰", "2327": "åœ‹å·¨*",
    "2880": "è¯å—é‡‘", "2887": "å°æ–°æ–°å…‰é‡‘", "5880": "åˆåº«é‡‘", "2890": "æ°¸è±é‡‘", "3045": "å°ç£å¤§",
    "4904": "é å‚³", "5871": "ä¸­ç§Ÿ-KY", "2324": "ä»å¯¶", "2301": "å…‰å¯¶ç§‘", "4938": "å’Œç¢©",
    "2409": "å‹é”", "3037": "æ¬£èˆˆ", "2344": "è¯é‚¦é›»", "2345": "æ™ºé‚¦", "6415": "çŸ½åŠ›*-KY",
    "3034": "è¯è© ", "2379": "ç‘æ˜±", "3231": "ç·¯å‰µ", "2408": "å—äºç§‘", "3661": "ä¸–èŠ¯-KY",
    "3443": "å‰µæ„", "2449": "äº¬å…ƒé›»å­", "2377": "å¾®æ˜Ÿ", "6669": "ç·¯ç©", "2474": "å¯æˆ",
    "1102": "äºæ³¥", "2105": "æ­£æ–°", "2201": "è£•éš†", "2227": "è£•æ—¥è»Š", "6176": "ç‘å„€",
    "9910": "è±æ³°", "2834": "è‡ºä¼éŠ€", "2801": "å½°éŠ€", "2049": "ä¸ŠéŠ€", "2353": "å®ç¢",
    "2354": "é´»æº–", "2356": "è‹±æ¥­é”", "3706": "ç¥é”", "5269": "ç¥¥ç¢©", "6239": "åŠ›æˆ",
    "6271": "åŒæ¬£é›»", "3481": "ç¾¤å‰µ", "4961": "å¤©éˆº", "2360": "è‡´èŒ‚", "2385": "ç¾¤å…‰",
    "2458": "ç¾©éš†", "3545": "æ•¦æ³°", "8046": "å—é›»", "2368": "é‡‘åƒé›»", "2371": "å¤§åŒ",
    "2376": "æŠ€å˜‰", "6282": "åº·èˆ’", "4968": "ç«‹ç©", "3017": "å¥‡é‹", "2347": "è¯å¼·",
    "2393": "å„„å…‰", "6116": "å½©æ™¶", "8150": "å—èŒ‚", "2313": "è¯é€š", "2337": "æ—ºå®",
    "6414": "æ¨ºæ¼¢", "2107": "åšç”Ÿ", "2204": "ä¸­è¯", "2362": "è—å¤©", "2427": "ä¸‰å•†é›»",
    "5876": "ä¸Šæµ·å•†éŠ€", "9945": "æ½¤æ³°æ–°", "2101": "å—æ¸¯", "2364": "å€«é£›", "3702": "å¤§è¯å¤§"
}


def push_image(original_url: str, preview_url: str):
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {
        "to": LINE_USER_ID,
        "messages": [{
            "type": "image",
            "originalContentUrl": original_url,
            "previewImageUrl": preview_url
        }]
    }
    r = requests.post(url, headers=headers, json=body, timeout=30)
    r.raise_for_status()

def line_push_text(msg: str):
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {"to": LINE_USER_ID, "messages": [{"type": "text", "text": msg}]}
    r = requests.post(url, headers=headers, json=body, timeout=30)
    r.raise_for_status()


def get_drive_service():
    """å»ºç«‹ Google Drive API æœå‹™ï¼ˆä½¿ç”¨ Service Accountï¼‰"""
    try:
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– Service Account JSON
        sa_json_str = os.environ.get("GDRIVE_SERVICE_ACCOUNT")
        if not sa_json_str:
            print("âŒ æœªè¨­å®š GDRIVE_SERVICE_ACCOUNT ç’°å¢ƒè®Šæ•¸ï¼Œè·³é Google Drive åŠŸèƒ½")
            return None

        sa_json = json.loads(sa_json_str)
        credentials = service_account.Credentials.from_service_account_info(sa_json, scopes=SCOPES)
        service = build('drive', 'v3', credentials=credentials)
        print("âœ… Google Drive Service Account èªè­‰æˆåŠŸ")
        return service
    except Exception as e:
        print(f"âŒ Google Drive èªè­‰å¤±æ•—: {e}")
        return None


def find_folder(service, folder_name, parent_id=None):
    """å°‹æ‰¾æŒ‡å®šåç¨±çš„è³‡æ–™å¤¾"""
    if not service:
        return None

    try:
        query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false"
        if parent_id:
            query += f" and '{parent_id}' in parents"

        results = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
        items = results.get('files', [])

        if items:
            return items[0]['id']
        return None
    except Exception as e:
        print(f"âŒ å°‹æ‰¾è³‡æ–™å¤¾å¤±æ•—: {e}")
        return None


def create_folder(service, folder_name, parent_id=None):
    """å»ºç«‹è³‡æ–™å¤¾"""
    if not service:
        return None

    try:
        file_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        if parent_id:
            file_metadata['parents'] = [parent_id]

        folder = service.files().create(body=file_metadata, fields='id').execute()
        print(f"âœ… å·²å»ºç«‹è³‡æ–™å¤¾: {folder_name}")
        return folder.get('id')
    except Exception as e:
        print(f"âŒ å»ºç«‹è³‡æ–™å¤¾å¤±æ•—: {e}")
        return None


def download_file_from_drive(service, file_name, folder_id, local_path):
    """å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆ"""
    if not service:
        return False

    try:
        # å°‹æ‰¾æª”æ¡ˆ
        query = f"name='{file_name}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields='files(id, name)').execute()
        items = results.get('files', [])

        if not items:
            print(f"ğŸ“ Google Drive ä¸­æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_name}")
            return False

        file_id = items[0]['id']

        # ä¸‹è¼‰æª”æ¡ˆ
        request = service.files().get_media(fileId=file_id)
        file_buffer = io.BytesIO()
        downloader = MediaIoBaseDownload(file_buffer, request)

        done = False
        while done is False:
            status, done = downloader.next_chunk()

        # å¯«å…¥æœ¬åœ°æª”æ¡ˆ
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, 'wb') as f:
            f.write(file_buffer.getvalue())

        print(f"âœ… å·²å¾ Google Drive ä¸‹è¼‰: {file_name}")
        return True

    except Exception as e:
        print(f"âŒ ä¸‹è¼‰æª”æ¡ˆå¤±æ•—: {e}")
        return False


def upload_file_to_drive(service, local_path, file_name, folder_id):
    """ä¸Šå‚³æª”æ¡ˆåˆ° Google Drive"""
    if not service:
        return False

    try:
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
        query = f"name='{file_name}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields='files(id, name)').execute()
        items = results.get('files', [])

        file_metadata = {'name': file_name, 'parents': [folder_id]}
        media = MediaFileUpload(local_path, resumable=True)

        if items:
            # æ›´æ–°ç¾æœ‰æª”æ¡ˆ
            file_id = items[0]['id']
            file = service.files().update(fileId=file_id, media_body=media).execute()
            print(f"âœ… å·²æ›´æ–° Google Drive æª”æ¡ˆ: {file_name}")
        else:
            # å»ºç«‹æ–°æª”æ¡ˆ
            file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()
            print(f"âœ… å·²ä¸Šå‚³æ–°æª”æ¡ˆåˆ° Google Drive: {file_name}")

        return True

    except Exception as e:
        print(f"âŒ ä¸Šå‚³æª”æ¡ˆå¤±æ•—: {e}")
        return False


def setup_google_drive_folders(service):
    """è¨­å®š Google Drive è³‡æ–™å¤¾çµæ§‹"""
    if not service:
        return None

    try:
        # å°‹æ‰¾æˆ–å»ºç«‹ä¸»è³‡æ–™å¤¾ stocks-autobot-data
        main_folder_id = find_folder(service, GDRIVE_FOLDER_NAME)
        if not main_folder_id:
            main_folder_id = create_folder(service, GDRIVE_FOLDER_NAME)

        if not main_folder_id:
            print("âŒ ç„¡æ³•å»ºç«‹ä¸»è³‡æ–™å¤¾")
            return None

        # å°‹æ‰¾æˆ–å»ºç«‹ data å­è³‡æ–™å¤¾
        data_folder_id = find_folder(service, GDRIVE_DATA_FOLDER, main_folder_id)
        if not data_folder_id:
            data_folder_id = create_folder(service, GDRIVE_DATA_FOLDER, main_folder_id)

        return data_folder_id

    except Exception as e:
        print(f"âŒ è¨­å®š Google Drive è³‡æ–™å¤¾å¤±æ•—: {e}")
        return None


def sync_database_from_drive(service):
    """å¾ Google Drive åŒæ­¥è³‡æ–™åº«åˆ°æœ¬åœ°"""
    if not service:
        print("âš ï¸  è·³é Google Drive ä¸‹è¼‰ï¼ˆService ä¸å¯ç”¨ï¼‰")
        return False

    try:
        data_folder_id = setup_google_drive_folders(service)
        if not data_folder_id:
            return False

        # ä¸‹è¼‰ taiex.sqlite
        success = download_file_from_drive(service, "taiex.sqlite", data_folder_id, DB_PATH)
        return success

    except Exception as e:
        print(f"âŒ å¾ Google Drive åŒæ­¥è³‡æ–™åº«å¤±æ•—: {e}")
        return False


def sync_database_to_drive(service):
    """ä¸Šå‚³æœ¬åœ°è³‡æ–™åº«åˆ° Google Drive"""
    if not service:
        print("âš ï¸  è·³é Google Drive ä¸Šå‚³ï¼ˆService ä¸å¯ç”¨ï¼‰")
        return False

    try:
        if not os.path.exists(DB_PATH):
            print(f"âš ï¸  æœ¬åœ°è³‡æ–™åº«ä¸å­˜åœ¨: {DB_PATH}")
            return False

        data_folder_id = setup_google_drive_folders(service)
        if not data_folder_id:
            return False

        # ä¸Šå‚³ taiex.sqlite
        success = upload_file_to_drive(service, DB_PATH, "taiex.sqlite", data_folder_id)
        return success

    except Exception as e:
        print(f"âŒ ä¸Šå‚³è³‡æ–™åº«åˆ° Google Drive å¤±æ•—: {e}")
        return False


def ensure_db():
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS prices(
                code TEXT,
                date TEXT,
                open REAL, high REAL, low REAL, close REAL,
                volume INTEGER,
                PRIMARY KEY(code, date)
            )
            """
        )
        conn.commit()


def get_existing_data_range() -> dict:
    if not os.path.exists(DB_PATH):
        return {}
    with sqlite3.connect(DB_PATH) as conn:
        cursor = conn.execute(
            "SELECT code, MIN(date) as min_date, MAX(date) as max_date FROM prices GROUP BY code"
        )
        result = {}
        for row in cursor:
            result[row[0]] = {"min": row[1], "max": row[2]}
    return result


def fetch_prices_yf(codes, lookback_days=120) -> pd.DataFrame:
    existing = get_existing_data_range()
    target_start = (datetime.utcnow() - timedelta(days=lookback_days * 2)).date().isoformat()

    codes_to_fetch = []
    for c in codes:
        c = c.strip()
        if not c:
            continue
        if c not in existing:
            codes_to_fetch.append(c)
            print(f"{c}: ç„¡æ­·å²è³‡æ–™ï¼Œéœ€ä¸‹è¼‰")
        else:
            max_date = existing[c]["max"]
            if max_date < (datetime.utcnow() - timedelta(days=2)).date().isoformat():
                codes_to_fetch.append(c)
                print(f"{c}: è³‡æ–™éèˆŠ (æœ€æ–°: {max_date})ï¼Œéœ€æ›´æ–°")
            else:
                print(f"{c}: è³‡æ–™å·²æ˜¯æœ€æ–° (æœ€æ–°: {max_date})")

    if not codes_to_fetch:
        print("æ‰€æœ‰è‚¡ç¥¨è³‡æ–™éƒ½å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€ä¸‹è¼‰")
        return pd.DataFrame()

    tickers = [f"{c}.TW" for c in codes_to_fetch]
    print(f"\né–‹å§‹ä¸‹è¼‰ {len(codes_to_fetch)} æ”¯è‚¡ç¥¨")
    print(f"æœŸé–“: {target_start} ~ ä»Šæ—¥")

    df = yf.download(
        tickers=" ".join(tickers),
        start=target_start,
        interval="1d",
        group_by="ticker",
        auto_adjust=False,
        progress=False,
    )
    out = []
    for c in codes_to_fetch:
        t = f"{c}.TW"
        if isinstance(df, pd.DataFrame) and t in df:
            tmp = df[t].reset_index().rename(columns=str.lower)
            if "date" in tmp.columns:
                tmp["date"] = pd.to_datetime(tmp["date"]).dt.tz_localize(None)
            tmp["code"] = c
            out.append(tmp[["code", "date", "open", "high", "low", "close", "volume"]])
    result = pd.concat(out, ignore_index=True) if out else pd.DataFrame()
    print(f"æˆåŠŸä¸‹è¼‰ {len(result)} ç­†æ•¸æ“š")
    return result


def upsert_prices(df: pd.DataFrame):
    if df.empty:
        return
    df = df.copy()
    df["date"] = pd.to_datetime(df["date"]).dt.date.astype(str)
    with sqlite3.connect(DB_PATH) as conn:
        df.to_sql("_prices_in", conn, if_exists="replace", index=False)
        conn.execute("DELETE FROM prices WHERE (code, date) IN (SELECT code, date FROM _prices_in)")
        conn.execute(
            """
            INSERT INTO prices(code, date, open, high, low, close, volume)
            SELECT code, date, open, high, low, close, volume FROM _prices_in
            """
        )
        conn.execute("DROP TABLE _prices_in")
        conn.commit()
    print(f"æ•¸æ“šå·²å­˜å…¥è³‡æ–™åº«: {DB_PATH}")


def load_recent_prices(days=120) -> pd.DataFrame:
    with sqlite3.connect(DB_PATH) as conn:
        df = pd.read_sql_query(
            "SELECT code, date, open, high, low, close, volume FROM prices",
            conn,
            parse_dates=["date"],
        )
    cutoff = datetime.utcnow() - timedelta(days=days)
    return df[df["date"] >= cutoff]


def pick_stocks(prices: pd.DataFrame, top_k=30) -> pd.DataFrame:
    if prices.empty:
        return pd.DataFrame()
    prices = prices.sort_values(["code", "date"])

    def add_feat(g):
        g = g.copy()
        g["ma20"] = g["close"].rolling(20, min_periods=20).mean()
        return g

    feat = prices.groupby("code", group_keys=False).apply(add_feat)

    results = []
    for code, group in feat.groupby("code"):
        group = group.sort_values("date")
        if len(group) < 5:
            continue

        last_5 = group.tail(5)
        if last_5["ma20"].isna().any():
            continue

        open_above = (last_5["open"] > last_5["ma20"]).all()
        close_above = (last_5["close"] > last_5["ma20"]).all()

        if not (open_above and close_above):
            continue

        ma20_values = last_5["ma20"].values
        ma20_slope = (ma20_values[-1] - ma20_values[0]) / 4

        if ma20_slope >= 1:
            continue
        price_std = last_5["close"].std()
        price_mean = last_5["close"].mean()
        volatility_pct = (price_std / price_mean * 100) if price_mean > 0 else 999
        if volatility_pct > 3.0:
            continue

        max_distance_allowed = max(2.0, volatility_pct * 1.5)

        min_price = last_5[["open", "close"]].min(axis=1)
        distance_pct = ((min_price - last_5["ma20"]) / last_5["ma20"] * 100)
        avg_distance = distance_pct.mean()

        if avg_distance > max_distance_allowed:
            continue

        avg_price = (last_5["open"] + last_5["close"]) / 2
        avg_ma20_distance = abs(avg_price - last_5["ma20"]).mean()

        latest = last_5.iloc[-1]
        is_lowest_close = latest["close"] == last_5["close"].min()

        results.append({
            "code": code,
            "close": latest["close"],
            "ma20": latest["ma20"],
            "distance": avg_distance,
            "volatility": volatility_pct,
            "ma20_slope": ma20_slope,
            "max_distance": max_distance_allowed,
            "volume": latest["volume"],
            "avg_ma20_distance": avg_ma20_distance,
            "is_lowest_close": is_lowest_close
        })

    if not results:
        return pd.DataFrame()

    result_df = pd.DataFrame(results)

    group1 = result_df[(result_df["ma20_slope"] >= 0.5) & (result_df["ma20_slope"] < 1)]
    group2 = result_df[result_df["ma20_slope"] < 0.5]

    if len(group1) > 6:
        group1_filtered = group1[group1["is_lowest_close"] == False]
        if len(group1_filtered) > 6:
            group1 = group1_filtered.nsmallest(6, "avg_ma20_distance")
        elif len(group1_filtered) > 0:
            group1 = group1_filtered
        else:
            group1 = group1.nsmallest(6, "avg_ma20_distance")

    if len(group2) > 6:
        group2_filtered = group2[group2["is_lowest_close"] == False]
        if len(group2_filtered) > 6:
            group2 = group2_filtered.nsmallest(6, "avg_ma20_distance")
        elif len(group2_filtered) > 0:
            group2 = group2_filtered
        else:
            group2 = group2.nsmallest(6, "avg_ma20_distance")

    final_result = pd.concat([group1, group2], ignore_index=True)
    return final_result


def plot_candlestick(ax, stock_data):
    """åœ¨æŒ‡å®šçš„ ax ä¸Šç¹ªè£½ K æ£’åœ–"""
    stock_data = stock_data.copy().reset_index(drop=True)

    for idx, row in stock_data.iterrows():
        date_num = idx
        open_price = row['open']
        high_price = row['high']
        low_price = row['low']
        close_price = row['close']

        color = '#FF4444' if close_price >= open_price else '#00AA00'

        ax.plot([date_num, date_num], [low_price, high_price], color=color, linewidth=0.8)

        body_height = abs(close_price - open_price)
        body_bottom = min(open_price, close_price)
        ax.bar(date_num, body_height, bottom=body_bottom, color=color, width=0.6, alpha=0.8)


def plot_stock_charts(codes: list, prices: pd.DataFrame) -> str:
    """ç¹ªè£½æœ€å¤š 6 æ”¯è‚¡ç¥¨çš„ K æ£’åœ–ï¼ˆ2x3 å­åœ–ï¼‰"""
    codes = codes[:6]
    n_stocks = len(codes)
    if n_stocks == 0:
        return None

    plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial']
    plt.rcParams['axes.unicode_minus'] = False

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()

    for i, code in enumerate(codes):
        stock_data = prices[prices["code"] == code].sort_values("date").tail(90)

        if stock_data.empty or len(stock_data) < 20:
            axes[i].text(0.5, 0.5, f"{code} {STOCK_NAMES.get(code, '')}\næ•¸æ“šä¸è¶³",
                        ha='center', va='center', fontsize=14)
            axes[i].set_xticks([])
            axes[i].set_yticks([])
            continue

        stock_data = stock_data.copy()
        stock_data["ma20"] = stock_data["close"].rolling(20, min_periods=20).mean()

        ax = axes[i]
        plot_candlestick(ax, stock_data)

        valid_ma20 = stock_data[stock_data["ma20"].notna()]
        if not valid_ma20.empty:
            ma20_indices = valid_ma20.index - stock_data.index[0]
            ax.plot(ma20_indices, valid_ma20["ma20"], label="MA20",
                   linewidth=2, linestyle="--", alpha=0.7, color='#2E86DE')

        stock_name = STOCK_NAMES.get(code, code)
        ax.set_title(f"{code} {stock_name}", fontsize=14, fontweight='bold', pad=10)
        ax.legend(fontsize=9, loc='upper left')
        ax.grid(True, alpha=0.3, linestyle='--')

        date_labels = stock_data["date"].dt.strftime('%m/%d').tolist()
        step = max(1, len(date_labels) // 6)
        tick_positions = list(range(0, len(date_labels), step))
        tick_labels = [date_labels[i] for i in tick_positions]
        ax.set_xticks(tick_positions)
        ax.set_xticklabels(tick_labels, rotation=45, fontsize=9)
        ax.tick_params(axis='y', labelsize=9)

    for i in range(n_stocks, 6):
        fig.delaxes(axes[i])

    plt.tight_layout()

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
    plt.savefig(temp_file.name, dpi=100, bbox_inches='tight')
    plt.close()

    return temp_file.name


def upload_to_telegraph(image_path: str) -> str:
    """ä½¿ç”¨ Telegraph ä¸Šå‚³åœ–ç‰‡ï¼ˆç„¡éœ€ API keyï¼‰"""
    try:
        with open(image_path, 'rb') as f:
            response = requests.post(
                'https://telegra.ph/upload',
                files={'file': ('image.png', f, 'image/png')},
                timeout=30
            )

        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                path = result[0].get('src')
                if path:
                    return f"https://telegra.ph{path}"

        print(f"Telegraph ä¸Šå‚³å¤±æ•—: {response.status_code} - {response.text[:200]}")
    except Exception as e:
        print(f"Telegraph ä¸Šå‚³ç•°å¸¸: {e}")
    return None


def upload_to_catbox(image_path: str) -> str:
    """ä½¿ç”¨ Catbox ä¸Šå‚³åœ–ç‰‡ï¼ˆç„¡éœ€ API keyï¼Œå‚™ç”¨ï¼‰"""
    try:
        with open(image_path, 'rb') as f:
            response = requests.post(
                'https://catbox.moe/user/api.php',
                data={'reqtype': 'fileupload'},
                files={'fileToUpload': f},
                timeout=30
            )

        if response.status_code == 200:
            url = response.text.strip()
            if url.startswith('https://'):
                return url

        print(f"Catbox ä¸Šå‚³å¤±æ•—: {response.status_code} - {response.text[:200]}")
    except Exception as e:
        print(f"Catbox ä¸Šå‚³ç•°å¸¸: {e}")
    return None


def upload_image(image_path: str) -> str:
    """å˜—è©¦å¤šå€‹åœ–åºŠä¸Šå‚³ï¼Œè¿”å›ç¬¬ä¸€å€‹æˆåŠŸçš„ URL"""
    print(f"å˜—è©¦ä¸Šå‚³åœ–ç‰‡: {image_path}")

    url = upload_to_telegraph(image_path)
    if url:
        print(f"âœ… Telegraph ä¸Šå‚³æˆåŠŸ: {url}")
        return url

    print("â†’ å˜—è©¦å‚™ç”¨åœ–åºŠ Catbox...")
    url = upload_to_catbox(image_path)
    if url:
        print(f"âœ… Catbox ä¸Šå‚³æˆåŠŸ: {url}")
        return url

    print("âŒ æ‰€æœ‰åœ–åºŠä¸Šå‚³å¤±æ•—")
    return None


def main():
    print("=== å°è‚¡æ¨è–¦æ©Ÿå™¨äººè‡ªå‹•åŸ·è¡Œ ===\n")

    print("æ­¥é©Ÿ 1: è¨­å®š Google Drive é€£ç·š")
    drive_service = get_drive_service()

    print("\næ­¥é©Ÿ 2: å¾ Google Drive åŒæ­¥è³‡æ–™åº«")
    sync_database_from_drive(drive_service)

    print("\næ­¥é©Ÿ 3: å»ºç«‹è³‡æ–™åº«")
    ensure_db()

    print("\næ­¥é©Ÿ 4: æª¢æŸ¥ä¸¦ä¸‹è¼‰éœ€è¦çš„æ•¸æ“š")
    df_new = fetch_prices_yf(CODES, lookback_days=120)
    data_updated = False
    if not df_new.empty:
        upsert_prices(df_new)
        data_updated = True
        print("âœ… è³‡æ–™åº«å·²æ›´æ–°")
    else:
        print("ç„¡éœ€æ›´æ–°è³‡æ–™åº«")

    print("\næ­¥é©Ÿ 5: è¼‰å…¥æ•¸æ“šä¸¦ç¯©é¸è‚¡ç¥¨")
    hist = load_recent_prices(days=120)
    picks = pick_stocks(hist, top_k=PICKS_TOP_K)

    print("\næ­¥é©Ÿ 6: å°‡è‚¡ç¥¨åˆ†çµ„")
    today_tpe = datetime.now(timezone(timedelta(hours=8))).date()

    if picks.empty:
        group1 = pd.DataFrame()
        group2 = pd.DataFrame()
    else:
        group1 = picks[(picks["ma20_slope"] >= 0.5) & (picks["ma20_slope"] < 1)]
        group2 = picks[picks["ma20_slope"] < 0.5]

    print(f"å¥½åƒè »å¼·çš„ï¼ˆæ–œç‡ 0.5-1ï¼‰ï¼š{len(group1)} æ”¯")
    print(f"æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ï¼ˆæ–œç‡ < 0.5ï¼‰ï¼š{len(group2)} æ”¯")

    print("\næ­¥é©Ÿ 7: ç™¼é€ LINE è¨Šæ¯")

    if group1.empty and group2.empty:
        msg = f"ğŸ“‰ {today_tpe}\nä»Šæ—¥ç„¡ç¬¦åˆæ¢ä»¶ä¹‹å°è‚¡æ¨è–¦ã€‚"
        print(f"\nå°‡ç™¼é€çš„è¨Šæ¯:\n{msg}\n")
        try:
            line_push_text(msg)
            print("âœ… LINE è¨Šæ¯ç™¼é€æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ LINE è¨Šæ¯ç™¼é€å¤±æ•—: {e}")
    else:
        if not group1.empty:
            print("\nè™•ç†ã€Œå¥½åƒè »å¼·çš„ã€çµ„...")
            lines = [f"ğŸ’ª å¥½åƒè »å¼·çš„ ({today_tpe})"]
            lines.append("ä»¥ä¸‹è‚¡ç¥¨å¯ä»¥åƒè€ƒï¼š\n")
            for i, r in group1.iterrows():
                stock_name = STOCK_NAMES.get(r.code, r.code)
                lines.append(f"{r.code} {stock_name}")
            msg1 = "\n".join(lines)
            print(f"è¨Šæ¯:\n{msg1}\n")

            try:
                line_push_text(msg1)
                print("âœ… å¥½åƒè »å¼·çš„çµ„è¨Šæ¯ç™¼é€æˆåŠŸ")
            except Exception as e:
                print(f"âŒ å¥½åƒè »å¼·çš„çµ„è¨Šæ¯ç™¼é€å¤±æ•—: {e}")

            print("\nç”Ÿæˆä¸¦ç™¼é€ã€Œå¥½åƒè »å¼·çš„ã€çµ„åœ–ç‰‡")
            group1_codes = group1["code"].tolist()
            for batch_num in range(0, len(group1_codes), 6):
                batch_codes = group1_codes[batch_num:batch_num + 6]
                batch_display = ", ".join(batch_codes)
                print(f"æ­£åœ¨è™•ç†å¥½åƒè »å¼·çš„ç¬¬ {batch_num//6 + 1} çµ„: {batch_display}")

                chart_path = plot_stock_charts(batch_codes, hist)
                if chart_path:
                    img_url = upload_image(chart_path)
                    if img_url:
                        try:
                            push_image(img_url, img_url)
                            print(f"âœ… åœ–è¡¨å·²ç™¼é€åˆ° LINE")
                        except Exception as e:
                            print(f"âŒ LINE ç™¼é€å¤±æ•—: {e}")
                        os.unlink(chart_path)
                    else:
                        print(f"âŒ åœ–åºŠä¸Šå‚³å¤±æ•—")
                else:
                    print(f"âŒ åœ–è¡¨ç”Ÿæˆå¤±æ•—")

        if not group2.empty:
            print("\nè™•ç†ã€Œæœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ã€çµ„...")
            lines = [f"ğŸ‘€ æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ ({today_tpe})"]
            lines.append("ä»¥ä¸‹è‚¡ç¥¨å¯ä»¥åƒè€ƒï¼š\n")
            for i, r in group2.iterrows():
                stock_name = STOCK_NAMES.get(r.code, r.code)
                lines.append(f"{r.code} {stock_name}")
            msg2 = "\n".join(lines)
            print(f"è¨Šæ¯:\n{msg2}\n")

            try:
                line_push_text(msg2)
                print("âœ… æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹çµ„è¨Šæ¯ç™¼é€æˆåŠŸ")
            except Exception as e:
                print(f"âŒ æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹çµ„è¨Šæ¯ç™¼é€å¤±æ•—: {e}")

            print("\nç”Ÿæˆä¸¦ç™¼é€ã€Œæœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ã€çµ„åœ–ç‰‡")
            group2_codes = group2["code"].tolist()
            for batch_num in range(0, len(group2_codes), 6):
                batch_codes = group2_codes[batch_num:batch_num + 6]
                batch_display = ", ".join(batch_codes)
                print(f"æ­£åœ¨è™•ç†æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ç¬¬ {batch_num//6 + 1} çµ„: {batch_display}")

                chart_path = plot_stock_charts(batch_codes, hist)
                if chart_path:
                    img_url = upload_image(chart_path)
                    if img_url:
                        try:
                            push_image(img_url, img_url)
                            print(f"âœ… åœ–è¡¨å·²ç™¼é€åˆ° LINE")
                        except Exception as e:
                            print(f"âŒ LINE ç™¼é€å¤±æ•—: {e}")
                        os.unlink(chart_path)
                    else:
                        print(f"âŒ åœ–åºŠä¸Šå‚³å¤±æ•—")
                else:
                    print(f"âŒ åœ–è¡¨ç”Ÿæˆå¤±æ•—")

    # æ­¥é©Ÿ 8: åŒæ­¥è³‡æ–™åº«åˆ° Google Driveï¼ˆå¦‚æœæœ‰æ›´æ–°è³‡æ–™ï¼‰
    if data_updated and drive_service:
        print("\næ­¥é©Ÿ 8: åŒæ­¥è³‡æ–™åº«åˆ° Google Drive")
        sync_database_to_drive(drive_service)
    elif drive_service:
        print("\næ­¥é©Ÿ 8: è³‡æ–™ç„¡æ›´æ–°ï¼Œè·³é Google Drive åŒæ­¥")
    else:
        print("\næ­¥é©Ÿ 8: Google Drive æœå‹™ä¸å¯ç”¨ï¼Œè·³éåŒæ­¥")

    print("\nğŸ‰ ä»»å‹™å®Œæˆï¼")


if __name__ == "__main__":
    main()