import os
import requests
import sqlite3
from datetime import datetime, timedelta, timezone
import tempfile
import base64
import json
import io

import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from dotenv import load_dotenv
from google.oauth2 import service_account
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload

load_dotenv()
LINE_TOKEN = os.environ["LINE_CHANNEL_ACCESS_TOKEN"]
LINE_USER_ID = os.environ["LINE_USER_ID"]

DATA_DIR = os.environ.get("DATA_DIR", "data")
DB_PATH = os.path.join(DATA_DIR, "taiex.sqlite")

# Google Drive Ë®≠ÂÆö - ÊîØÊè¥Áõ¥Êé•ÊåáÂÆöË≥áÊñôÂ§æ ID Êàñ‰ΩøÁî®È†êË®≠ÂêçÁ®±ÊêúÂ∞ã
GDRIVE_FOLDER_ID = os.environ.get("GDRIVE_FOLDER_ID")
GOOGLE_DRIVE_FOLDER_ID = os.environ.get("GOOGLE_DRIVE_FOLDER_ID", GDRIVE_FOLDER_ID)  # OAuth ÁâàÊú¨Áõ∏ÂÆπ
GDRIVE_FOLDER_NAME = "stocks-autobot-data"  # È†êË®≠Ë≥áÊñôÂ§æÂêçÁ®±ÔºàÂÇôÁî®Ôºâ
GDRIVE_DATA_FOLDER = "data"  # Âú®‰∏ªË≥áÊñôÂ§æ‰∏ãÁöÑÂ≠êË≥áÊñôÂ§æ

# Google Drive Ë™çË≠âË®≠ÂÆöÔºàÊîØÊè¥ÂÖ©Á®ÆÊñπÂºèÔºâ
OAUTH_CREDENTIALS = os.environ.get("OAUTH")  # OAuth 2.0 Ë™çË≠â
GDRIVE_SERVICE_ACCOUNT = os.environ.get("GDRIVE_SERVICE_ACCOUNT")  # Service Account Ë™çË≠â
SCOPES = ['https://www.googleapis.com/auth/drive']

# Use environment variable for stock codes, fallback to comprehensive list
DEFAULT_CODES = [
    "1101", "1102", "1103", "1104", "1108", "1109", "1110", "1201", "1203", "1210",
    "1213", "1215", "1216", "1217", "1218", "1219", "1220", "1225", "1227", "1229",
    "1231", "1232", "1233", "1234", "1235", "1236", "1256", "1301", "1303", "1304",
    "1305", "1307", "1308", "1309", "1310", "1312", "1313", "1314", "1315", "1316",
    "1319", "1321", "1323", "1324", "1325", "1326", "1337", "1338", "1339", "1340",
    "1341", "1342", "1402", "1409", "1410", "1413", "1414", "1416", "1417", "1418",
    "1419", "1423", "1432", "1434", "1435", "1436", "1437", "1438", "1439", "1440",
    "1441", "1442", "1443", "1444", "1445", "1446", "1447", "1449", "1451", "1452",
    "1453", "1454", "1455", "1456", "1457", "1459", "1460", "1463", "1464", "1465",
    "1466", "1467", "1468", "1470", "1471", "1472", "1473", "1474", "1475", "1476",
    "1477", "1503", "1504", "1506", "1512", "1513", "1514", "1515", "1516", "1517",
    "1519", "1521", "1522", "1524", "1525", "1526", "1527", "1528", "1529", "1530",
    "1531", "1532", "1533", "1535", "1536", "1537", "1538", "1539", "1540", "1541",
    "1558", "1560", "1563", "1568", "1582", "1583", "1587", "1589", "1590", "1597",
    "1598", "1603", "1604", "1605", "1608", "1609", "1611", "1612", "1614", "1615",
    "1616", "1617", "1618", "1626", "1702", "1707", "1708", "1709", "1710", "1711",
    "1712", "1713", "1714", "1717", "1718", "1720", "1721", "1722", "1723", "1725",
    "1726", "1727", "1730", "1731", "1732", "1733", "1734", "1735", "1736", "1737",
    "1752", "1760", "1762", "1773", "1776", "1783", "1786", "1789", "1795", "1802",
    "1805", "1806", "1808", "1809", "1810", "1817", "1903", "1904", "1905", "1906",
    "1907", "1909", "2002", "2006", "2007", "2008", "2009", "2010", "2012", "2013",
    "2014", "2015", "2017", "2020", "2022", "2023", "2024", "2025", "2027", "2028",
    "2029", "2030", "2031", "2032", "2033", "2034", "2038", "2049", "2059", "2062",
    "2069", "2101", "2102", "2103", "2104", "2105", "2106", "2107", "2108", "2109",
    "2114", "2115", "2201", "2204", "2206", "2207", "2208", "2211", "2227", "2228",
    "2231", "2233", "2236", "2239", "2241", "2243", "2247", "2248", "2250", "2301",
    "2302", "2303", "2305", "2308", "2312", "2313", "2314", "2316", "2317", "2321",
    "2323", "2324", "2327", "2328", "2329", "2330", "2331", "2332", "2337", "2338",
    "2340", "2342", "2344", "2345", "2347", "2348", "2349", "2351", "2352", "2353",
    "2354", "2355", "2356", "2357", "2359", "2360", "2362", "2363", "2364", "2365"
]

CODES = os.environ.get("TWSE_CODES", ",".join(DEFAULT_CODES)).split(",")
PICKS_TOP_K = int(os.environ.get("TOP_K", "300"))

STOCK_NAMES = {
    "1101": "Âè∞Ê≥•", "1102": "‰∫ûÊ≥•", "1103": "ÂòâÊ≥•", "1104": "Áí∞Ê≥•", "1108": "Âπ∏Á¶è",
    "1109": "‰ø°Â§ß", "1110": "Êù±Ê≥•", "1201": "Âë≥ÂÖ®", "1203": "Âë≥Áéã", "1210": "Â§ßÊàê",
    "1213": "Â§ßÈ£≤", "1215": "ÂçúËúÇ", "1216": "Áµ±‰∏Ä", "1217": "ÊÑõ‰πãÂë≥", "1218": "Ê≥∞Â±±",
    "1219": "Á¶èÂ£Ω", "1220": "Âè∞Ê¶Æ", "1225": "Á¶èÊáãÊ≤π", "1227": "‰Ω≥Ê†º", "1229": "ËÅØËèØ",
    "1231": "ËÅØËèØÈ£ü", "1232": "Â§ßÁµ±Áõä", "1233": "Â§©‰ªÅ", "1234": "ÈªëÊùæ", "1235": "ËààÊ≥∞",
    "1236": "ÂÆè‰∫û", "1256": "ÈÆÆÊ¥ªÊûúÊ±Å-KY", "1301": "Âè∞Â°ë", "1303": "Âçó‰∫û", "1304": "Âè∞ËÅö",
    "1305": "ËèØÂ§è", "1307": "‰∏âËä≥", "1308": "‰∫ûËÅö", "1309": "Âè∞ÈÅîÂåñ", "1310": "Âè∞ËãØ",
    "1312": "ÂúãÂñ¨", "1313": "ËÅØÊàê", "1314": "‰∏≠Áü≥Âåñ", "1315": "ÈÅîÊñ∞", "1316": "‰∏äÊõú",
    "1319": "Êù±ÈôΩ", "1321": "Â§ßÊ¥ã", "1323": "Ê∞∏Ë£ï", "1324": "Âú∞ÁêÉ", "1325": "ÊÅÜÂ§ß",
    "1326": "Âè∞Âåñ", "1337": "ÂÜçÁîü-KY", "1338": "Âª£ËèØ-KY", "1339": "Êò≠Ëºù", "1340": "ÂãùÊÇÖ-KY",
    "1341": "ÂØåÊûó-KY", "1342": "ÂÖ´Ë≤´", "1402": "ÈÅ†Êù±Êñ∞", "1409": "Êñ∞Á∫ñ", "1410": "ÂçóÊüì",
    "1413": "ÂÆèÊ¥≤", "1414": "Êù±Âíå", "1416": "Âª£Ë±ê", "1417": "ÂòâË£ï", "1418": "Êù±ËèØ",
    "1419": "Êñ∞Á¥°", "1423": "Âà©ËèØ", "1432": "Â§ßÈ≠ØÈñ£", "1434": "Á¶èÊáã", "1435": "‰∏≠Á¶è",
    "1436": "ËèØÂèãËÅØ", "1437": "Âã§ÁõäÊéß", "1438": "‰∏âÂú∞ÈñãÁôº", "1439": "ÈõãÊèö", "1440": "ÂçóÁ¥°",
    "1441": "Â§ßÊù±", "1442": "ÂêçËªí", "1443": "Á´ãÁõäÁâ©ÊµÅ", "1444": "ÂäõÈ∫ó", "1445": "Â§ßÂÆá",
    "1446": "ÂÆèÂíå", "1447": "ÂäõÈµ¨", "1449": "‰Ω≥Âíå", "1451": "Âπ¥Ëàà", "1452": "ÂÆèÁõä",
    "1453": "Â§ßÂ∞á", "1454": "Âè∞ÂØå", "1455": "ÈõÜÁõõ", "1456": "ÊÄ°ËèØ", "1457": "ÂÆúÈÄ≤",
    "1459": "ËÅØÁôº", "1460": "ÂÆèÈÅ†", "1463": "Âº∑ÁõõÊñ∞", "1464": "ÂæóÂäõ", "1465": "ÂÅâÂÖ®",
    "1466": "ËÅöÈöÜ", "1467": "ÂçóÁ∑Ø", "1468": "Êò∂Âíå", "1470": "Â§ßÁµ±Êñ∞Ââµ", "1471": "È¶ñÂà©",
    "1472": "‰∏âÊ¥ãÂØ¶Ê•≠", "1473": "Âè∞Âçó", "1474": "ÂºòË£ï", "1475": "Ê•≠Êó∫", "1476": "ÂÑíÈ¥ª",
    "1477": "ËÅöÈôΩ", "1503": "Â£´Èõª", "1504": "Êù±ÂÖÉ", "1506": "Ê≠£ÈÅì", "1512": "ÁëûÂà©",
    "1513": "‰∏≠ËààÈõª", "1514": "‰∫ûÂäõ", "1515": "ÂäõÂ±±", "1516": "Â∑ùÈ£õ", "1517": "Âà©Â•á",
    "1519": "ËèØÂüé", "1521": "Â§ßÂÑÑ", "1522": "Â†§Á∂≠Ë•ø", "1524": "ËÄøÈºé", "1525": "Ê±üÁî≥",
    "1526": "Êó•È¶≥", "1527": "ÈëΩÂÖ®", "1528": "ÊÅ©Âæ∑", "1529": "Ê®Ç‰∫ãÁ∂†ËÉΩ", "1530": "‰∫ûÂ¥¥",
    "1531": "È´òÊûóËÇ°", "1532": "Âã§Áæé", "1533": "ËªäÁéãÈõª", "1535": "‰∏≠ÂÆá", "1536": "ÂíåÂ§ß",
    "1537": "Âª£ÈöÜ", "1538": "Ê≠£Â≥∞", "1539": "Â∑®Â∫≠", "1540": "Âñ¨Á¶è", "1541": "Èå©Ê≥∞",
    "1558": "‰º∏Ëàà", "1560": "‰∏≠Á†Ç", "1563": "Â∑ßÊñ∞", "1568": "ÂÄâ‰Ωë", "1582": "‰ø°Èå¶",
    "1583": "Á®ãÊ≥∞", "1587": "ÂêâËåÇ", "1589": "Ê∞∏ÂÜ†-KY", "1590": "‰∫ûÂæ∑ÂÆ¢-KY", "1597": "Áõ¥Âæó",
    "1598": "Â≤±ÂÆá", "1603": "ËèØÈõª", "1604": "ËÅ≤ÂØ∂", "1605": "ËèØÊñ∞", "1608": "ËèØÊ¶Æ",
    "1609": "Â§ß‰∫û", "1611": "‰∏≠Èõª", "1612": "ÂÆèÊ≥∞", "1614": "‰∏âÊ¥ãÈõª", "1615": "Â§ßÂ±±",
    "1616": "ÂÑÑÊ≥∞", "1617": "Ê¶ÆÊòü", "1618": "ÂêàÊ©ü", "1626": "ËâæÁæéÁâπ-KY", "1702": "ÂçóÂÉë",
    "1707": "Ëë°ËêÑÁéã", "1708": "Êù±Èπº", "1709": "ÂíåÁõä", "1710": "Êù±ËÅØ", "1711": "Ê∞∏ÂÖâ",
    "1712": "ËààËæ≤", "1713": "ÂúãÂåñ", "1714": "ÂíåÊ°ê", "1717": "Èï∑Ëàà", "1718": "‰∏≠Á∫ñ",
    "1720": "ÁîüÈÅî", "1721": "‰∏âÊôÉ", "1722": "Âè∞ËÇ•", "1723": "‰∏≠Á¢≥", "1725": "ÂÖÉÁ¶é",
    "1726": "Ê∞∏Ë®ò", "1727": "‰∏≠ËèØÂåñ", "1730": "Ëä±‰ªôÂ≠ê", "1731": "ÁæéÂêæËèØ", "1732": "ÊØõÂØ∂",
    "1733": "‰∫îÈºé", "1734": "ÊùèËºù", "1735": "Êó•ÂãùÂåñ", "1736": "Âñ¨Â±±", "1737": "Ëá∫ÈπΩ",
    "1752": "ÂçóÂÖâ", "1760": "ÂØ∂ÈΩ°ÂØåÈå¶", "1762": "‰∏≠ÂåñÁîü", "1773": "Âãù‰∏Ä", "1776": "Â±ïÂÆá",
    "1783": "ÂíåÂ∫∑Áîü", "1786": "ÁßëÂ¶ç", "1789": "Á•ûÈöÜ", "1795": "ÁæéÊôÇ", "1802": "Âè∞Áéª",
    "1805": "ÂØ∂Âæ†", "1806": "ÂÜ†Ëªç", "1808": "ÊΩ§ÈöÜ", "1809": "‰∏≠Èáâ", "1810": "ÂíåÊàê",
    "1817": "Âá±ÊííË°õ", "1903": "Â£´Á¥ô", "1904": "Ê≠£ÈöÜ", "1905": "ËèØÁ¥ô", "1906": "ÂØ∂ÈöÜ",
    "1907": "Ê∞∏Ë±êÈ§ò", "1909": "Ê¶ÆÊàê", "2002": "‰∏≠Èãº", "2006": "Êù±ÂíåÈãºÈêµ", "2007": "ÁáÅËàà",
    "2008": "È´òËààÊòå", "2009": "Á¨¨‰∏ÄÈäÖ", "2010": "Êò•Ê∫ê", "2012": "Êò•Èõ®", "2013": "‰∏≠ÈãºÊßã",
    "2014": "‰∏≠È¥ª", "2015": "Ë±êËàà", "2017": "ÂÆòÁî∞Èãº", "2020": "Áæé‰∫û", "2022": "ËÅö‰∫®",
    "2023": "ÁáÅËºù", "2024": "ÂøóËÅØ", "2025": "ÂçÉËàà", "2027": "Â§ßÊàêÈãº", "2028": "Â®ÅËá¥",
    "2029": "ÁõõÈ§ò", "2030": "ÂΩ∞Ê∫ê", "2031": "Êñ∞ÂÖâÈãº", "2032": "Êñ∞Èãº", "2033": "‰Ω≥Â§ß",
    "2034": "ÂÖÅÂº∑", "2038": "Êµ∑ÂÖâ", "2049": "‰∏äÈäÄ", "2059": "Â∑ùÊπñ", "2062": "Ê©ãÊ§ø",
    "2069": "ÈÅãÈå©", "2101": "ÂçóÊ∏Ø", "2102": "Ê≥∞Ë±ê", "2103": "Âè∞Ê©°", "2104": "ÂúãÈöõ‰∏≠Ê©°",
    "2105": "Ê≠£Êñ∞", "2106": "Âª∫Â§ß", "2107": "ÂéöÁîü", "2108": "ÂçóÂ∏ù", "2109": "ËèØË±ê",
    "2114": "Èë´Ê∞∏Èäì", "2115": "ÂÖ≠Êöâ-KY", "2201": "Ë£ïÈöÜ", "2204": "‰∏≠ËèØ", "2206": "‰∏âÈôΩÂ∑•Ê•≠",
    "2207": "ÂíåÊ≥∞Ëªä", "2208": "Âè∞Ëàπ", "2211": "Èï∑Ê¶ÆÈãº", "2227": "Ë£ïÊó•Ëªä", "2228": "ÂäçÈ∫ü",
    "2231": "ÁÇ∫Âçá", "2233": "ÂÆáÈöÜ", "2236": "ÁôæÈÅî-KY", "2239": "Ëã±Âà©-KY", "2241": "ËâæÂßÜÂãí",
    "2243": "ÂÆèÊó≠-KY", "2247": "Ê±éÂæ∑Ê∞∏Ê•≠", "2248": "ËèØÂãù-KY", "2250": "IKKA-KY", "2301": "ÂÖâÂØ∂Áßë",
    "2302": "È∫óÊ≠£", "2303": "ËÅØÈõª", "2305": "ÂÖ®Âèã", "2308": "Âè∞ÈÅîÈõª", "2312": "ÈáëÂØ∂",
    "2313": "ËèØÈÄö", "2314": "Âè∞Êèö", "2316": "Ê•†Ê¢ìÈõª", "2317": "È¥ªÊµ∑", "2321": "Êù±Ë®ä",
    "2323": "‰∏≠Áí∞", "2324": "‰ªÅÂØ∂", "2327": "ÂúãÂ∑®*", "2328": "Âª£ÂÆá", "2329": "ËèØÊ≥∞",
    "2330": "Âè∞Á©çÈõª", "2331": "Á≤æËã±", "2332": "ÂèãË®ä", "2337": "Êó∫ÂÆè", "2338": "ÂÖâÁΩ©",
    "2340": "Âè∞‰∫û", "2342": "ËåÇÁüΩ", "2344": "ËèØÈÇ¶Èõª", "2345": "Êô∫ÈÇ¶", "2347": "ËÅØÂº∑",
    "2348": "Êµ∑ÊÇÖ", "2349": "Èå∏Âæ∑", "2351": "È†ÜÂæ∑", "2352": "‰Ω≥‰∏ñÈÅî", "2353": "ÂÆèÁ¢Å",
    "2354": "È¥ªÊ∫ñ", "2355": "Êï¨Èµ¨", "2356": "Ëã±Ê•≠ÈÅî", "2357": "ËèØÁ¢©", "2359": "ÊâÄÁæÖÈñÄ",
    "2360": "Ëá¥ËåÇ", "2362": "ËóçÂ§©", "2363": "ÁüΩÁµ±", "2364": "ÂÄ´È£õ", "2365": "ÊòÜÁõà"
}


def push_image(original_url: str, preview_url: str):
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {
        "to": LINE_USER_ID,
        "messages": [{
            "type": "image",
            "originalContentUrl": original_url,
            "previewImageUrl": preview_url
        }]
    }
    r = requests.post(url, headers=headers, json=body, timeout=30)
    r.raise_for_status()

def line_push_text(msg: str):
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {"to": LINE_USER_ID, "messages": [{"type": "text", "text": msg}]}
    r = requests.post(url, headers=headers, json=body, timeout=30)
    r.raise_for_status()


def get_drive_service():
    """Âª∫Á´ã Google Drive API ÊúçÂãôÔºà‰ΩøÁî® OAuth 2.0Ôºâ"""
    if not OAUTH_CREDENTIALS:
        raise ValueError("Êú™Ë®≠ÂÆö OAUTH Áí∞Â¢ÉËÆäÊï∏ÔºåÁÑ°Ê≥ïÈÄ≤Ë°å Google Drive Ë™çË≠â")

    try:
        print("üîê Google Drive OAuth 2.0 Ë™çË≠â...")
        oauth_data = json.loads(OAUTH_CREDENTIALS)

        creds = Credentials(
            token=oauth_data.get('token'),
            refresh_token=oauth_data.get('refresh_token'),
            token_uri=oauth_data.get('token_uri'),
            client_id=oauth_data.get('client_id'),
            client_secret=oauth_data.get('client_secret'),
            scopes=SCOPES
        )

        if creds.expired and creds.refresh_token:
            print("üîÑ ÈáçÊñ∞Êï¥ÁêÜ Google Drive ÊéàÊ¨ä...")
            creds.refresh(Request())

        service = build('drive', 'v3', credentials=creds)
        print("‚úÖ Google Drive OAuth Ë™çË≠âÊàêÂäü")
        return service
    except Exception as e:
        print(f"‚ùå OAuth Ë™çË≠âÂ§±Êïó: {e}")
        raise


def find_folder(service, folder_name, parent_id=None):
    """Â∞ãÊâæÊåáÂÆöÂêçÁ®±ÁöÑË≥áÊñôÂ§æ"""
    if not service:
        return None

    try:
        query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false"
        if parent_id:
            query += f" and '{parent_id}' in parents"

        results = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
        items = results.get('files', [])

        if items:
            return items[0]['id']
        return None
    except Exception as e:
        print(f"‚ùå Â∞ãÊâæË≥áÊñôÂ§æÂ§±Êïó: {e}")
        return None


def create_folder(service, folder_name, parent_id=None):
    """Âª∫Á´ãË≥áÊñôÂ§æ"""
    if not service:
        return None

    try:
        file_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        if parent_id:
            file_metadata['parents'] = [parent_id]

        folder = service.files().create(body=file_metadata, fields='id').execute()
        print(f"‚úÖ Â∑≤Âª∫Á´ãË≥áÊñôÂ§æ: {folder_name}")
        return folder.get('id')
    except Exception as e:
        print(f"‚ùå Âª∫Á´ãË≥áÊñôÂ§æÂ§±Êïó: {e}")
        return None


def download_file_from_drive(service, file_name, folder_id, local_path):
    """Âæû Google Drive ‰∏ãËºâÊ™îÊ°à"""
    if not service:
        return False

    try:
        # Â∞ãÊâæÊ™îÊ°à
        query = f"name='{file_name}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields='files(id, name)').execute()
        items = results.get('files', [])

        if not items:
            print(f"üìÅ Google Drive ‰∏≠Êâæ‰∏çÂà∞Ê™îÊ°à: {file_name}")
            return False

        file_id = items[0]['id']

        # ‰∏ãËºâÊ™îÊ°à
        request = service.files().get_media(fileId=file_id)
        file_buffer = io.BytesIO()
        downloader = MediaIoBaseDownload(file_buffer, request)

        done = False
        while done is False:
            status, done = downloader.next_chunk()

        # ÂØ´ÂÖ•Êú¨Âú∞Ê™îÊ°à
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, 'wb') as f:
            f.write(file_buffer.getvalue())

        print(f"‚úÖ Â∑≤Âæû Google Drive ‰∏ãËºâ: {file_name}")
        return True

    except Exception as e:
        print(f"‚ùå ‰∏ãËºâÊ™îÊ°àÂ§±Êïó: {e}")
        return False


def upload_file_to_drive(service, local_path, file_name, folder_id):
    """‰∏äÂÇ≥Ê™îÊ°àÂà∞ Google Drive"""
    if not service:
        return False

    try:
        # Ê™¢Êü•Ê™îÊ°àÊòØÂê¶Â∑≤Â≠òÂú®
        query = f"name='{file_name}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields='files(id, name)').execute()
        items = results.get('files', [])

        file_metadata = {'name': file_name, 'parents': [folder_id]}
        media = MediaFileUpload(local_path, resumable=True)

        if items:
            # Êõ¥Êñ∞ÁèæÊúâÊ™îÊ°à
            file_id = items[0]['id']
            file = service.files().update(fileId=file_id, media_body=media).execute()
            print(f"‚úÖ Â∑≤Êõ¥Êñ∞ Google Drive Ê™îÊ°à: {file_name}")
        else:
            # Âª∫Á´ãÊñ∞Ê™îÊ°à
            file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()
            print(f"‚úÖ Â∑≤‰∏äÂÇ≥Êñ∞Ê™îÊ°àÂà∞ Google Drive: {file_name}")

        return True

    except Exception as e:
        print(f"‚ùå ‰∏äÂÇ≥Ê™îÊ°àÂ§±Êïó: {e}")
        return False


def upload_to_google_drive(file_path: str, filename: str, folder_id: str, mimetype: str = 'image/png') -> str:
    """‰∏äÂÇ≥Ê™îÊ°àÂà∞ Google Drive ‰∏¶ËøîÂõûÂàÜ‰∫´ÈÄ£Áµê"""
    try:
        service = get_drive_service()

        file_metadata = {
            'name': filename,
            'parents': [folder_id]
        }

        media = MediaFileUpload(file_path, mimetype=mimetype, resumable=True)
        file = service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id, webViewLink'
        ).execute()

        service.permissions().create(
            fileId=file.get('id'),
            body={'type': 'anyone', 'role': 'reader'}
        ).execute()

        return file.get('webViewLink')
    except Exception as e:
        print(f"Google Drive ‰∏äÂÇ≥Â§±Êïó: {e}")
        return None


def upload_text_to_google_drive(text_content: str, filename: str, folder_id: str) -> str:
    """Â∞áÊñáÂ≠óÂÖßÂÆπÂ≠òÁÇ∫ txt ‰∏¶‰∏äÂÇ≥Âà∞ Google Drive"""
    try:
        temp_file = tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix='.txt')
        temp_file.write(text_content)
        temp_file.close()

        result = upload_to_google_drive(temp_file.name, filename, folder_id, mimetype='text/plain')

        os.unlink(temp_file.name)
        return result
    except Exception as e:
        print(f"‰∏äÂÇ≥ÊñáÂ≠óÊ™îÂ§±Êïó: {e}")
        return None


def setup_google_drive_folders(service):
    """Ë®≠ÂÆö Google Drive Ë≥áÊñôÂ§æÁµêÊßã"""
    if not service:
        return None

    try:
        # Â¶ÇÊûúÊúâÁõ¥Êé•ÊåáÂÆöË≥áÊñôÂ§æ IDÔºåÂÑ™ÂÖà‰ΩøÁî®ÔºàÊîØÊè¥ÂÖ©Á®ÆËÆäÊï∏ÂêçÁ®±Ôºâ
        folder_id = GOOGLE_DRIVE_FOLDER_ID or GDRIVE_FOLDER_ID
        if folder_id:
            print(f"‚úÖ ‰ΩøÁî®ÊåáÂÆöÁöÑ Google Drive Ë≥áÊñôÂ§æ ID: {folder_id}")
            main_folder_id = folder_id
        else:
            # Â∞ãÊâæÊàñÂª∫Á´ã‰∏ªË≥áÊñôÂ§æ stocks-autobot-data
            print(f"üîç ÊêúÂ∞ãË≥áÊñôÂ§æ: {GDRIVE_FOLDER_NAME}")
            main_folder_id = find_folder(service, GDRIVE_FOLDER_NAME)
            if not main_folder_id:
                main_folder_id = create_folder(service, GDRIVE_FOLDER_NAME)

            if not main_folder_id:
                print("‚ùå ÁÑ°Ê≥ïÂª∫Á´ã‰∏ªË≥áÊñôÂ§æ")
                return None

        # Â∞ãÊâæÊàñÂª∫Á´ã data Â≠êË≥áÊñôÂ§æ
        data_folder_id = find_folder(service, GDRIVE_DATA_FOLDER, main_folder_id)
        if not data_folder_id:
            data_folder_id = create_folder(service, GDRIVE_DATA_FOLDER, main_folder_id)

        if data_folder_id:
            print(f"‚úÖ Google Drive Ë≥áÊñôÂ§æÂ∑≤Ê∫ñÂÇôÂ∞±Á∑í: {GDRIVE_DATA_FOLDER}")

        return data_folder_id

    except Exception as e:
        print(f"‚ùå Ë®≠ÂÆö Google Drive Ë≥áÊñôÂ§æÂ§±Êïó: {e}")
        return None


def sync_database_from_drive(service):
    """Âæû Google Drive ÂêåÊ≠•Ë≥áÊñôÂ∫´Âà∞Êú¨Âú∞"""
    if not service:
        print("‚ö†Ô∏è  Ë∑≥ÈÅé Google Drive ‰∏ãËºâÔºàService ‰∏çÂèØÁî®Ôºâ")
        return False

    try:
        data_folder_id = setup_google_drive_folders(service)
        if not data_folder_id:
            return False

        # ‰∏ãËºâ taiex.sqlite
        success = download_file_from_drive(service, "taiex.sqlite", data_folder_id, DB_PATH)
        return success

    except Exception as e:
        print(f"‚ùå Âæû Google Drive ÂêåÊ≠•Ë≥áÊñôÂ∫´Â§±Êïó: {e}")
        return False


def sync_database_to_drive(service):
    """‰∏äÂÇ≥Êú¨Âú∞Ë≥áÊñôÂ∫´Âà∞ Google Drive"""
    if not service:
        print("‚ö†Ô∏è  Ë∑≥ÈÅé Google Drive ‰∏äÂÇ≥ÔºàService ‰∏çÂèØÁî®Ôºâ")
        return False

    try:
        if not os.path.exists(DB_PATH):
            print(f"‚ö†Ô∏è  Êú¨Âú∞Ë≥áÊñôÂ∫´‰∏çÂ≠òÂú®: {DB_PATH}")
            return False

        data_folder_id = setup_google_drive_folders(service)
        if not data_folder_id:
            return False

        # ‰∏äÂÇ≥ taiex.sqlite
        success = upload_file_to_drive(service, DB_PATH, "taiex.sqlite", data_folder_id)
        return success

    except Exception as e:
        print(f"‚ùå ‰∏äÂÇ≥Ë≥áÊñôÂ∫´Âà∞ Google Drive Â§±Êïó: {e}")
        return False


def ensure_db():
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS prices(
                code TEXT,
                date TEXT,
                open REAL, high REAL, low REAL, close REAL,
                volume INTEGER,
                PRIMARY KEY(code, date)
            )
            """
        )
        conn.commit()


def get_existing_data_range() -> dict:
    if not os.path.exists(DB_PATH):
        return {}
    with sqlite3.connect(DB_PATH) as conn:
        cursor = conn.execute(
            "SELECT code, MIN(date) as min_date, MAX(date) as max_date FROM prices GROUP BY code"
        )
        result = {}
        for row in cursor:
            result[row[0]] = {"min": row[1], "max": row[2]}
    return result


def fetch_prices_yf(codes, lookback_days=120) -> pd.DataFrame:
    existing = get_existing_data_range()
    target_start = (datetime.utcnow() - timedelta(days=lookback_days * 2)).date().isoformat()

    codes_to_fetch = []
    for c in codes:
        c = c.strip()
        if not c:
            continue
        if c not in existing:
            codes_to_fetch.append(c)
            print(f"{c}: ÁÑ°Ê≠∑Âè≤Ë≥áÊñôÔºåÈúÄ‰∏ãËºâ")
        else:
            max_date = existing[c]["max"]
            if max_date < (datetime.utcnow() - timedelta(days=2)).date().isoformat():
                codes_to_fetch.append(c)
                print(f"{c}: Ë≥áÊñôÈÅéËàä (ÊúÄÊñ∞: {max_date})ÔºåÈúÄÊõ¥Êñ∞")
            else:
                print(f"{c}: Ë≥áÊñôÂ∑≤ÊòØÊúÄÊñ∞ (ÊúÄÊñ∞: {max_date})")

    if not codes_to_fetch:
        print("ÊâÄÊúâËÇ°Á•®Ë≥áÊñôÈÉΩÂ∑≤ÊòØÊúÄÊñ∞ÔºåÁÑ°ÈúÄ‰∏ãËºâ")
        return pd.DataFrame()

    tickers = [f"{c}.TW" for c in codes_to_fetch]
    print(f"\nÈñãÂßã‰∏ãËºâ {len(codes_to_fetch)} ÊîØËÇ°Á•®")
    print(f"ÊúüÈñì: {target_start} ~ ‰ªäÊó•")

    df = yf.download(
        tickers=" ".join(tickers),
        start=target_start,
        interval="1d",
        group_by="ticker",
        auto_adjust=False,
        progress=False,
    )
    out = []
    for c in codes_to_fetch:
        t = f"{c}.TW"
        if isinstance(df, pd.DataFrame) and t in df:
            tmp = df[t].reset_index().rename(columns=str.lower)
            if "date" in tmp.columns:
                tmp["date"] = pd.to_datetime(tmp["date"]).dt.tz_localize(None)
            tmp["code"] = c
            out.append(tmp[["code", "date", "open", "high", "low", "close", "volume"]])
    result = pd.concat(out, ignore_index=True) if out else pd.DataFrame()
    print(f"ÊàêÂäü‰∏ãËºâ {len(result)} Á≠ÜÊï∏Êìö")
    return result


def upsert_prices(df: pd.DataFrame):
    if df.empty:
        return
    df = df.copy()
    df["date"] = pd.to_datetime(df["date"]).dt.date.astype(str)
    with sqlite3.connect(DB_PATH) as conn:
        df.to_sql("_prices_in", conn, if_exists="replace", index=False)
        conn.execute("DELETE FROM prices WHERE (code, date) IN (SELECT code, date FROM _prices_in)")
        conn.execute(
            """
            INSERT INTO prices(code, date, open, high, low, close, volume)
            SELECT code, date, open, high, low, close, volume FROM _prices_in
            """
        )
        conn.execute("DROP TABLE _prices_in")
        conn.commit()
    print(f"Êï∏ÊìöÂ∑≤Â≠òÂÖ•Ë≥áÊñôÂ∫´: {DB_PATH}")


def load_recent_prices(days=120) -> pd.DataFrame:
    with sqlite3.connect(DB_PATH) as conn:
        df = pd.read_sql_query(
            "SELECT code, date, open, high, low, close, volume FROM prices",
            conn,
            parse_dates=["date"],
        )
    cutoff = datetime.utcnow() - timedelta(days=days)
    return df[df["date"] >= cutoff]


def pick_stocks(prices: pd.DataFrame, top_k=30) -> pd.DataFrame:
    if prices.empty:
        return pd.DataFrame()
    prices = prices.sort_values(["code", "date"])

    def add_feat(g):
        g = g.copy()
        g["ma20"] = g["close"].rolling(20, min_periods=20).mean()
        return g

    feat = prices.groupby("code", group_keys=False).apply(add_feat)

    results = []
    for code, group in feat.groupby("code"):
        group = group.sort_values("date")
        if len(group) < 5:
            continue

        last_5 = group.tail(5)
        if last_5["ma20"].isna().any():
            continue

        open_above = (last_5["open"] > last_5["ma20"]).all()
        close_above = (last_5["close"] > last_5["ma20"]).all()

        if not (open_above and close_above):
            continue

        ma20_values = last_5["ma20"].values
        ma20_slope = (ma20_values[-1] - ma20_values[0]) / 4

        if ma20_slope >= 1:
            continue
        price_std = last_5["close"].std()
        price_mean = last_5["close"].mean()
        volatility_pct = (price_std / price_mean * 100) if price_mean > 0 else 999
        if volatility_pct > 3.0:
            continue

        max_distance_allowed = max(2.0, volatility_pct * 1.5)

        min_price = last_5[["open", "close"]].min(axis=1)
        distance_pct = ((min_price - last_5["ma20"]) / last_5["ma20"] * 100)
        avg_distance = distance_pct.mean()

        if avg_distance > max_distance_allowed:
            continue

        avg_price = (last_5["open"] + last_5["close"]) / 2
        avg_ma20_distance = abs(avg_price - last_5["ma20"]).mean()

        latest = last_5.iloc[-1]
        is_lowest_close = latest["close"] == last_5["close"].min()

        results.append({
            "code": code,
            "close": latest["close"],
            "ma20": latest["ma20"],
            "distance": avg_distance,
            "volatility": volatility_pct,
            "ma20_slope": ma20_slope,
            "max_distance": max_distance_allowed,
            "volume": latest["volume"],
            "avg_ma20_distance": avg_ma20_distance,
            "is_lowest_close": is_lowest_close
        })

    if not results:
        return pd.DataFrame()

    result_df = pd.DataFrame(results)

    group1 = result_df[(result_df["ma20_slope"] >= 0.5) & (result_df["ma20_slope"] < 1)]
    group2 = result_df[result_df["ma20_slope"] < 0.5]

    if len(group1) > 6:
        group1_filtered = group1[group1["is_lowest_close"] == False]
        if len(group1_filtered) > 6:
            group1 = group1_filtered.nsmallest(6, "avg_ma20_distance")
        elif len(group1_filtered) > 0:
            group1 = group1_filtered
        else:
            group1 = group1.nsmallest(6, "avg_ma20_distance")

    if len(group2) > 6:
        group2_filtered = group2[group2["is_lowest_close"] == False]
        if len(group2_filtered) > 6:
            group2 = group2_filtered.nsmallest(6, "avg_ma20_distance")
        elif len(group2_filtered) > 0:
            group2 = group2_filtered
        else:
            group2 = group2.nsmallest(6, "avg_ma20_distance")

    final_result = pd.concat([group1, group2], ignore_index=True)
    return final_result


def plot_candlestick(ax, stock_data):
    """Âú®ÊåáÂÆöÁöÑ ax ‰∏äÁπ™Ë£Ω K Ê£íÂúñ"""
    stock_data = stock_data.copy().reset_index(drop=True)

    for idx, row in stock_data.iterrows():
        date_num = idx
        open_price = row['open']
        high_price = row['high']
        low_price = row['low']
        close_price = row['close']

        color = '#FF4444' if close_price >= open_price else '#00AA00'

        ax.plot([date_num, date_num], [low_price, high_price], color=color, linewidth=0.8)

        body_height = abs(close_price - open_price)
        body_bottom = min(open_price, close_price)
        ax.bar(date_num, body_height, bottom=body_bottom, color=color, width=0.6, alpha=0.8)


def plot_stock_charts(codes: list, prices: pd.DataFrame) -> str:
    """Áπ™Ë£ΩÊúÄÂ§ö 6 ÊîØËÇ°Á•®ÁöÑ K Ê£íÂúñÔºà2x3 Â≠êÂúñÔºâ"""
    codes = codes[:6]
    n_stocks = len(codes)
    if n_stocks == 0:
        return None

    # Ë®≠ÂÆöÂ≠óÈ´îÂÑ™ÂÖàÁ¥öÔºöWindowsÂ≠óÈ´î -> LinuxÂ≠óÈ´î -> ÈÄöÁî®Â≠óÈ´î
    plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'WenQuanYi Zen Hei', 'WenQuanYi Micro Hei', 'DejaVu Sans', 'Arial']
    plt.rcParams['axes.unicode_minus'] = False

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()

    for i, code in enumerate(codes):
        stock_data = prices[prices["code"] == code].sort_values("date").tail(90)

        if stock_data.empty or len(stock_data) < 20:
            axes[i].text(0.5, 0.5, f"{code} {STOCK_NAMES.get(code, '')}\nÊï∏Êìö‰∏çË∂≥",
                        ha='center', va='center', fontsize=14)
            axes[i].set_xticks([])
            axes[i].set_yticks([])
            continue

        stock_data = stock_data.copy()
        stock_data["ma20"] = stock_data["close"].rolling(20, min_periods=20).mean()

        ax = axes[i]
        plot_candlestick(ax, stock_data)

        valid_ma20 = stock_data[stock_data["ma20"].notna()]
        if not valid_ma20.empty:
            ma20_indices = valid_ma20.index - stock_data.index[0]
            ax.plot(ma20_indices, valid_ma20["ma20"], label="MA20",
                   linewidth=2, linestyle="--", alpha=0.7, color='#2E86DE')

        stock_name = STOCK_NAMES.get(code, code)
        ax.set_title(f"{code} {stock_name}", fontsize=14, fontweight='bold', pad=10)
        ax.legend(fontsize=9, loc='upper left')
        ax.grid(True, alpha=0.3, linestyle='--')

        date_labels = stock_data["date"].dt.strftime('%m/%d').tolist()
        step = max(1, len(date_labels) // 6)
        tick_positions = list(range(0, len(date_labels), step))
        tick_labels = [date_labels[i] for i in tick_positions]
        ax.set_xticks(tick_positions)
        ax.set_xticklabels(tick_labels, rotation=45, fontsize=9)
        ax.tick_params(axis='y', labelsize=9)

    for i in range(n_stocks, 6):
        fig.delaxes(axes[i])

    plt.tight_layout()

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
    plt.savefig(temp_file.name, dpi=100, bbox_inches='tight')
    plt.close()

    return temp_file.name


def upload_to_telegraph(image_path: str) -> str:
    """‰ΩøÁî® Telegraph ‰∏äÂÇ≥ÂúñÁâáÔºàÁÑ°ÈúÄ API keyÔºâ"""
    try:
        with open(image_path, 'rb') as f:
            response = requests.post(
                'https://telegra.ph/upload',
                files={'file': ('image.png', f, 'image/png')},
                timeout=30
            )

        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                path = result[0].get('src')
                if path:
                    return f"https://telegra.ph{path}"

        print(f"Telegraph ‰∏äÂÇ≥Â§±Êïó: {response.status_code} - {response.text[:200]}")
    except Exception as e:
        print(f"Telegraph ‰∏äÂÇ≥Áï∞Â∏∏: {e}")
    return None


def upload_to_catbox(image_path: str) -> str:
    """‰ΩøÁî® Catbox ‰∏äÂÇ≥ÂúñÁâáÔºàÁÑ°ÈúÄ API keyÔºåÂÇôÁî®Ôºâ"""
    try:
        with open(image_path, 'rb') as f:
            response = requests.post(
                'https://catbox.moe/user/api.php',
                data={'reqtype': 'fileupload'},
                files={'fileToUpload': f},
                timeout=30
            )

        if response.status_code == 200:
            url = response.text.strip()
            if url.startswith('https://'):
                return url

        print(f"Catbox ‰∏äÂÇ≥Â§±Êïó: {response.status_code} - {response.text[:200]}")
    except Exception as e:
        print(f"Catbox ‰∏äÂÇ≥Áï∞Â∏∏: {e}")
    return None


def upload_image(image_path: str) -> str:
    """ÂòóË©¶Â§öÂÄãÂúñÂ∫ä‰∏äÂÇ≥ÔºåËøîÂõûÁ¨¨‰∏ÄÂÄãÊàêÂäüÁöÑ URL"""
    print(f"ÂòóË©¶‰∏äÂÇ≥ÂúñÁâá: {image_path}")

    url = upload_to_telegraph(image_path)
    if url:
        print(f"‚úÖ Telegraph ‰∏äÂÇ≥ÊàêÂäü: {url}")
        return url

    print("‚Üí ÂòóË©¶ÂÇôÁî®ÂúñÂ∫ä Catbox...")
    url = upload_to_catbox(image_path)
    if url:
        print(f"‚úÖ Catbox ‰∏äÂÇ≥ÊàêÂäü: {url}")
        return url

    print("‚ùå ÊâÄÊúâÂúñÂ∫ä‰∏äÂÇ≥Â§±Êïó")
    return None


def main():
    print("=== Âè∞ËÇ°Êé®Ëñ¶Ê©üÂô®‰∫∫Ëá™ÂãïÂü∑Ë°å ===\n")

    print("Ê≠•È©ü 1: Ë®≠ÂÆö Google Drive ÈÄ£Á∑ö")
    drive_service = get_drive_service()

    print("\nÊ≠•È©ü 2: Âæû Google Drive ÂêåÊ≠•Ë≥áÊñôÂ∫´")
    sync_database_from_drive(drive_service)

    print("\nÊ≠•È©ü 3: Âª∫Á´ãË≥áÊñôÂ∫´")
    ensure_db()

    print("\nÊ≠•È©ü 4: Ê™¢Êü•‰∏¶‰∏ãËºâÈúÄË¶ÅÁöÑÊï∏Êìö")
    df_new = fetch_prices_yf(CODES, lookback_days=120)
    data_updated = False
    if not df_new.empty:
        upsert_prices(df_new)
        data_updated = True
        print("‚úÖ Ë≥áÊñôÂ∫´Â∑≤Êõ¥Êñ∞")
    else:
        print("ÁÑ°ÈúÄÊõ¥Êñ∞Ë≥áÊñôÂ∫´")

    print("\nÊ≠•È©ü 5: ËºâÂÖ•Êï∏Êìö‰∏¶ÁØ©ÈÅ∏ËÇ°Á•®")
    hist = load_recent_prices(days=120)
    picks = pick_stocks(hist, top_k=PICKS_TOP_K)

    print("\nÊ≠•È©ü 6: Â∞áËÇ°Á•®ÂàÜÁµÑ")
    today_tpe = datetime.now(timezone(timedelta(hours=8))).date()

    if picks.empty:
        group1 = pd.DataFrame()
        group2 = pd.DataFrame()
    else:
        group1 = picks[(picks["ma20_slope"] >= 0.5) & (picks["ma20_slope"] < 1)]
        group2 = picks[picks["ma20_slope"] < 0.5]

    print(f"Â•ΩÂÉèË†ªÂº∑ÁöÑÔºàÊñúÁéá 0.5-1ÔºâÔºö{len(group1)} ÊîØ")
    print(f"ÊúâÊ©üÊúÉÂô¥ ËßÄÂØü‰∏Ä‰∏ãÔºàÊñúÁéá < 0.5ÔºâÔºö{len(group2)} ÊîØ")

    print("\nÊ≠•È©ü 7: ÁôºÈÄÅ LINE Ë®äÊÅØ")

    if group1.empty and group2.empty:
        msg = f"üìâ {today_tpe}\n‰ªäÊó•ÁÑ°Á¨¶ÂêàÊ¢ù‰ª∂‰πãÂè∞ËÇ°Êé®Ëñ¶„ÄÇ"
        print(f"\nÂ∞áÁôºÈÄÅÁöÑË®äÊÅØ:\n{msg}\n")
        try:
            line_push_text(msg)
            print("‚úÖ LINE Ë®äÊÅØÁôºÈÄÅÊàêÂäüÔºÅ")
        except Exception as e:
            print(f"‚ùå LINE Ë®äÊÅØÁôºÈÄÅÂ§±Êïó: {e}")
    else:
        if not group1.empty:
            print("\nËôïÁêÜ„ÄåÂ•ΩÂÉèË†ªÂº∑ÁöÑ„ÄçÁµÑ...")
            lines = [f"üí™ Â•ΩÂÉèË†ªÂº∑ÁöÑ ({today_tpe})"]
            lines.append("‰ª•‰∏ãËÇ°Á•®ÂèØ‰ª•ÂèÉËÄÉÔºö\n")
            for i, r in group1.iterrows():
                stock_name = STOCK_NAMES.get(r.code, r.code)
                lines.append(f"{r.code} {stock_name}")
            msg1 = "\n".join(lines)
            print(f"Ë®äÊÅØ:\n{msg1}\n")

            try:
                line_push_text(msg1)
                print("‚úÖ Â•ΩÂÉèË†ªÂº∑ÁöÑÁµÑË®äÊÅØÁôºÈÄÅÊàêÂäü")
            except Exception as e:
                print(f"‚ùå Â•ΩÂÉèË†ªÂº∑ÁöÑÁµÑË®äÊÅØÁôºÈÄÅÂ§±Êïó: {e}")

            print("\nÁîüÊàê‰∏¶ÁôºÈÄÅ„ÄåÂ•ΩÂÉèË†ªÂº∑ÁöÑ„ÄçÁµÑÂúñÁâá")
            group1_codes = group1["code"].tolist()
            for batch_num in range(0, len(group1_codes), 6):
                batch_codes = group1_codes[batch_num:batch_num + 6]
                batch_display = ", ".join(batch_codes)
                print(f"Ê≠£Âú®ËôïÁêÜÂ•ΩÂÉèË†ªÂº∑ÁöÑÁ¨¨ {batch_num//6 + 1} ÁµÑ: {batch_display}")

                chart_path = plot_stock_charts(batch_codes, hist)
                if chart_path:
                    img_url = upload_image(chart_path)
                    if img_url:
                        try:
                            push_image(img_url, img_url)
                            print(f"‚úÖ ÂúñË°®Â∑≤ÁôºÈÄÅÂà∞ LINE")
                        except Exception as e:
                            print(f"‚ùå LINE ÁôºÈÄÅÂ§±Êïó: {e}")
                        os.unlink(chart_path)
                    else:
                        print(f"‚ùå ÂúñÂ∫ä‰∏äÂÇ≥Â§±Êïó")
                else:
                    print(f"‚ùå ÂúñË°®ÁîüÊàêÂ§±Êïó")

        if not group2.empty:
            print("\nËôïÁêÜ„ÄåÊúâÊ©üÊúÉÂô¥ ËßÄÂØü‰∏Ä‰∏ã„ÄçÁµÑ...")
            lines = [f"üëÄ ÊúâÊ©üÊúÉÂô¥ ËßÄÂØü‰∏Ä‰∏ã ({today_tpe})"]
            lines.append("‰ª•‰∏ãËÇ°Á•®ÂèØ‰ª•ÂèÉËÄÉÔºö\n")
            for i, r in group2.iterrows():
                stock_name = STOCK_NAMES.get(r.code, r.code)
                lines.append(f"{r.code} {stock_name}")
            msg2 = "\n".join(lines)
            print(f"Ë®äÊÅØ:\n{msg2}\n")

            try:
                line_push_text(msg2)
                print("‚úÖ ÊúâÊ©üÊúÉÂô¥ ËßÄÂØü‰∏Ä‰∏ãÁµÑË®äÊÅØÁôºÈÄÅÊàêÂäü")
            except Exception as e:
                print(f"‚ùå ÊúâÊ©üÊúÉÂô¥ ËßÄÂØü‰∏Ä‰∏ãÁµÑË®äÊÅØÁôºÈÄÅÂ§±Êïó: {e}")

            print("\nÁîüÊàê‰∏¶ÁôºÈÄÅ„ÄåÊúâÊ©üÊúÉÂô¥ ËßÄÂØü‰∏Ä‰∏ã„ÄçÁµÑÂúñÁâá")
            group2_codes = group2["code"].tolist()
            for batch_num in range(0, len(group2_codes), 6):
                batch_codes = group2_codes[batch_num:batch_num + 6]
                batch_display = ", ".join(batch_codes)
                print(f"Ê≠£Âú®ËôïÁêÜÊúâÊ©üÊúÉÂô¥ ËßÄÂØü‰∏Ä‰∏ãÁ¨¨ {batch_num//6 + 1} ÁµÑ: {batch_display}")

                chart_path = plot_stock_charts(batch_codes, hist)
                if chart_path:
                    img_url = upload_image(chart_path)
                    if img_url:
                        try:
                            push_image(img_url, img_url)
                            print(f"‚úÖ ÂúñË°®Â∑≤ÁôºÈÄÅÂà∞ LINE")
                        except Exception as e:
                            print(f"‚ùå LINE ÁôºÈÄÅÂ§±Êïó: {e}")
                        os.unlink(chart_path)
                    else:
                        print(f"‚ùå ÂúñÂ∫ä‰∏äÂÇ≥Â§±Êïó")
                else:
                    print(f"‚ùå ÂúñË°®ÁîüÊàêÂ§±Êïó")

    # Ê≠•È©ü 8: ÂêåÊ≠•Ë≥áÊñôÂ∫´Âà∞ Google DriveÔºàÂ¶ÇÊûúÊúâÊõ¥Êñ∞Ë≥áÊñôÔºâ
    if data_updated and drive_service:
        print("\nÊ≠•È©ü 8: ÂêåÊ≠•Ë≥áÊñôÂ∫´Âà∞ Google Drive")
        sync_database_to_drive(drive_service)
    elif drive_service:
        print("\nÊ≠•È©ü 8: Ë≥áÊñôÁÑ°Êõ¥Êñ∞ÔºåË∑≥ÈÅé Google Drive ÂêåÊ≠•")
    else:
        print("\nÊ≠•È©ü 8: Google Drive ÊúçÂãô‰∏çÂèØÁî®ÔºåË∑≥ÈÅéÂêåÊ≠•")

    print("\nüéâ ‰ªªÂãôÂÆåÊàêÔºÅ")


if __name__ == "__main__":
    main()