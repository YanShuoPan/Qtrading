import os
import requests
import sqlite3
from datetime import datetime, timedelta, timezone
import tempfile
import base64
import json
import io
import logging

import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from dotenv import load_dotenv
from google.oauth2 import service_account
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload

load_dotenv()

# Debugè¨­å®š
DEBUG_MODE = os.environ.get("DEBUG_MODE", "false").lower() == "true"

# è¨­å®šæ—¥èªŒ
if DEBUG_MODE:
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
        handlers=[
            logging.FileHandler('debug.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    print(f"ğŸ› DEBUGæ¨¡å¼å·²å•Ÿç”¨ï¼Œè©³ç´°æ—¥èªŒå°‡ä¿å­˜åˆ° debug.log")
else:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

logger = logging.getLogger(__name__)
logger.info("=== å°è‚¡æ¨è–¦æ©Ÿå™¨äººå•Ÿå‹• ===")
logger.info(f"DEBUG_MODE: {DEBUG_MODE}")

LINE_TOKEN = os.environ["LINE_CHANNEL_ACCESS_TOKEN"]
LINE_USER_ID = os.environ["LINE_USER_ID"]

DATA_DIR = os.environ.get("DATA_DIR", "data")
DB_PATH = os.path.join(DATA_DIR, "taiex.sqlite")

# Google Drive è¨­å®š - æ”¯æ´ç›´æ¥æŒ‡å®šè³‡æ–™å¤¾ ID æˆ–ä½¿ç”¨é è¨­åç¨±æœå°‹
GDRIVE_FOLDER_ID = os.environ.get("GDRIVE_FOLDER_ID")
GOOGLE_DRIVE_FOLDER_ID = os.environ.get("GOOGLE_DRIVE_FOLDER_ID", GDRIVE_FOLDER_ID)  # OAuth ç‰ˆæœ¬ç›¸å®¹
GDRIVE_FOLDER_NAME = "stocks-autobot-data"  # é è¨­è³‡æ–™å¤¾åç¨±ï¼ˆå‚™ç”¨ï¼‰
GDRIVE_DATA_FOLDER = "data"  # åœ¨ä¸»è³‡æ–™å¤¾ä¸‹çš„å­è³‡æ–™å¤¾

# Google Drive èªè­‰è¨­å®šï¼ˆæ”¯æ´å…©ç¨®æ–¹å¼ï¼‰
OAUTH_CREDENTIALS = os.environ.get("OAUTH")  # OAuth 2.0 èªè­‰
GDRIVE_SERVICE_ACCOUNT = os.environ.get("GDRIVE_SERVICE_ACCOUNT")  # Service Account èªè­‰
SCOPES = ['https://www.googleapis.com/auth/drive']

# Use environment variable for stock codes, fallback to comprehensive list
DEFAULT_CODES = [
    "1101", "1102", "1103", "1104", "1108", "1109", "1110", "1201", "1203", "1210",
    "1213", "1215", "1216", "1217", "1218", "1219", "1220", "1225", "1227", "1229",
    "1231", "1232", "1233", "1234", "1235", "1236", "1256", "1301", "1303", "1304",
    "1305", "1307", "1308", "1309", "1310", "1312", "1313", "1314", "1315", "1316",
    "1319", "1321", "1323", "1324", "1325", "1326", "1337", "1338", "1339", "1340",
    "1341", "1342", "1402", "1409", "1410", "1413", "1414", "1416", "1417", "1418",
    "1419", "1423", "1432", "1434", "1435", "1436", "1437", "1438", "1439", "1440",
    "1441", "1442", "1443", "1444", "1445", "1446", "1447", "1449", "1451", "1452",
    "1453", "1454", "1455", "1456", "1457", "1459", "1460", "1463", "1464", "1465",
    "1466", "1467", "1468", "1470", "1471", "1472", "1473", "1474", "1475", "1476",
    "1477", "1503", "1504", "1506", "1512", "1513", "1514", "1515", "1516", "1517",
    "1519", "1521", "1522", "1524", "1525", "1526", "1527", "1528", "1529", "1530",
    "1531", "1532", "1533", "1535", "1536", "1537", "1538", "1539", "1540", "1541",
    "1558", "1560", "1563", "1568", "1582", "1583", "1587", "1589", "1590", "1597",
    "1598", "1603", "1604", "1605", "1608", "1609", "1611", "1612", "1614", "1615",
    "1616", "1617", "1618", "1626", "1702", "1707", "1708", "1709", "1710", "1711",
    "1712", "1713", "1714", "1717", "1718", "1720", "1721", "1722", "1723", "1725",
    "1726", "1727", "1730", "1731", "1732", "1733", "1734", "1735", "1736", "1737",
    "1752", "1760", "1762", "1773", "1776", "1783", "1786", "1789", "1795", "1802",
    "1805", "1806", "1808", "1809", "1810", "1817", "1903", "1904", "1905", "1906",
    "1907", "1909", "2002", "2006", "2007", "2008", "2009", "2010", "2012", "2013",
    "2014", "2015", "2017", "2020", "2022", "2023", "2024", "2025", "2027", "2028",
    "2029", "2030", "2031", "2032", "2033", "2034", "2038", "2049", "2059", "2062",
    "2069", "2101", "2102", "2103", "2104", "2105", "2106", "2107", "2108", "2109",
    "2114", "2115", "2201", "2204", "2206", "2207", "2208", "2211", "2227", "2228",
    "2231", "2233", "2236", "2239", "2241", "2243", "2247", "2248", "2250", "2301",
    "2302", "2303", "2305", "2308", "2312", "2313", "2314", "2316", "2317", "2321",
    "2323", "2324", "2327", "2328", "2329", "2330", "2331", "2332", "2337", "2338",
    "2340", "2342", "2344", "2345", "2347", "2348", "2349", "2351", "2352", "2353",
    "2354", "2355", "2356", "2357", "2359", "2360", "2362", "2363", "2364", "2365"
]

CODES = os.environ.get("TWSE_CODES", ",".join(DEFAULT_CODES)).split(",")
PICKS_TOP_K = int(os.environ.get("TOP_K", "300"))

STOCK_NAMES = {
    "1101": "å°æ³¥", "1102": "äºæ³¥", "1103": "å˜‰æ³¥", "1104": "ç’°æ³¥", "1108": "å¹¸ç¦",
    "1109": "ä¿¡å¤§", "1110": "æ±æ³¥", "1201": "å‘³å…¨", "1203": "å‘³ç‹", "1210": "å¤§æˆ",
    "1213": "å¤§é£²", "1215": "åœèœ‚", "1216": "çµ±ä¸€", "1217": "æ„›ä¹‹å‘³", "1218": "æ³°å±±",
    "1219": "ç¦å£½", "1220": "å°æ¦®", "1225": "ç¦æ‡‹æ²¹", "1227": "ä½³æ ¼", "1229": "è¯è¯",
    "1231": "è¯è¯é£Ÿ", "1232": "å¤§çµ±ç›Š", "1233": "å¤©ä»", "1234": "é»‘æ¾", "1235": "èˆˆæ³°",
    "1236": "å®äº", "1256": "é®®æ´»æœæ±-KY", "1301": "å°å¡‘", "1303": "å—äº", "1304": "å°èš",
    "1305": "è¯å¤", "1307": "ä¸‰èŠ³", "1308": "äºèš", "1309": "å°é”åŒ–", "1310": "å°è‹¯",
    "1312": "åœ‹å–¬", "1313": "è¯æˆ", "1314": "ä¸­çŸ³åŒ–", "1315": "é”æ–°", "1316": "ä¸Šæ›œ",
    "1319": "æ±é™½", "1321": "å¤§æ´‹", "1323": "æ°¸è£•", "1324": "åœ°çƒ", "1325": "æ†å¤§",
    "1326": "å°åŒ–", "1337": "å†ç”Ÿ-KY", "1338": "å»£è¯-KY", "1339": "æ˜­è¼", "1340": "å‹æ‚…-KY",
    "1341": "å¯Œæ—-KY", "1342": "å…«è²«", "1402": "é æ±æ–°", "1409": "æ–°çº–", "1410": "å—æŸ“",
    "1413": "å®æ´²", "1414": "æ±å’Œ", "1416": "å»£è±", "1417": "å˜‰è£•", "1418": "æ±è¯",
    "1419": "æ–°ç´¡", "1423": "åˆ©è¯", "1432": "å¤§é­¯é–£", "1434": "ç¦æ‡‹", "1435": "ä¸­ç¦",
    "1436": "è¯å‹è¯", "1437": "å‹¤ç›Šæ§", "1438": "ä¸‰åœ°é–‹ç™¼", "1439": "é›‹æš", "1440": "å—ç´¡",
    "1441": "å¤§æ±", "1442": "åè»’", "1443": "ç«‹ç›Šç‰©æµ", "1444": "åŠ›éº—", "1445": "å¤§å®‡",
    "1446": "å®å’Œ", "1447": "åŠ›éµ¬", "1449": "ä½³å’Œ", "1451": "å¹´èˆˆ", "1452": "å®ç›Š",
    "1453": "å¤§å°‡", "1454": "å°å¯Œ", "1455": "é›†ç››", "1456": "æ€¡è¯", "1457": "å®œé€²",
    "1459": "è¯ç™¼", "1460": "å®é ", "1463": "å¼·ç››æ–°", "1464": "å¾—åŠ›", "1465": "å‰å…¨",
    "1466": "èšéš†", "1467": "å—ç·¯", "1468": "æ˜¶å’Œ", "1470": "å¤§çµ±æ–°å‰µ", "1471": "é¦–åˆ©",
    "1472": "ä¸‰æ´‹å¯¦æ¥­", "1473": "å°å—", "1474": "å¼˜è£•", "1475": "æ¥­æ—º", "1476": "å„’é´»",
    "1477": "èšé™½", "1503": "å£«é›»", "1504": "æ±å…ƒ", "1506": "æ­£é“", "1512": "ç‘åˆ©",
    "1513": "ä¸­èˆˆé›»", "1514": "äºåŠ›", "1515": "åŠ›å±±", "1516": "å·é£›", "1517": "åˆ©å¥‡",
    "1519": "è¯åŸ", "1521": "å¤§å„„", "1522": "å ¤ç¶­è¥¿", "1524": "è€¿é¼", "1525": "æ±Ÿç”³",
    "1526": "æ—¥é¦³", "1527": "é‘½å…¨", "1528": "æ©å¾·", "1529": "æ¨‚äº‹ç¶ èƒ½", "1530": "äºå´´",
    "1531": "é«˜æ—è‚¡", "1532": "å‹¤ç¾", "1533": "è»Šç‹é›»", "1535": "ä¸­å®‡", "1536": "å’Œå¤§",
    "1537": "å»£éš†", "1538": "æ­£å³°", "1539": "å·¨åº­", "1540": "å–¬ç¦", "1541": "éŒ©æ³°",
    "1558": "ä¼¸èˆˆ", "1560": "ä¸­ç ‚", "1563": "å·§æ–°", "1568": "å€‰ä½‘", "1582": "ä¿¡éŒ¦",
    "1583": "ç¨‹æ³°", "1587": "å‰èŒ‚", "1589": "æ°¸å† -KY", "1590": "äºå¾·å®¢-KY", "1597": "ç›´å¾—",
    "1598": "å²±å®‡", "1603": "è¯é›»", "1604": "è²å¯¶", "1605": "è¯æ–°", "1608": "è¯æ¦®",
    "1609": "å¤§äº", "1611": "ä¸­é›»", "1612": "å®æ³°", "1614": "ä¸‰æ´‹é›»", "1615": "å¤§å±±",
    "1616": "å„„æ³°", "1617": "æ¦®æ˜Ÿ", "1618": "åˆæ©Ÿ", "1626": "è‰¾ç¾ç‰¹-KY", "1702": "å—åƒ‘",
    "1707": "è‘¡è„ç‹", "1708": "æ±é¹¼", "1709": "å’Œç›Š", "1710": "æ±è¯", "1711": "æ°¸å…‰",
    "1712": "èˆˆè¾²", "1713": "åœ‹åŒ–", "1714": "å’Œæ¡", "1717": "é•·èˆˆ", "1718": "ä¸­çº–",
    "1720": "ç”Ÿé”", "1721": "ä¸‰æ™ƒ", "1722": "å°è‚¥", "1723": "ä¸­ç¢³", "1725": "å…ƒç¦",
    "1726": "æ°¸è¨˜", "1727": "ä¸­è¯åŒ–", "1730": "èŠ±ä»™å­", "1731": "ç¾å¾è¯", "1732": "æ¯›å¯¶",
    "1733": "äº”é¼", "1734": "æè¼", "1735": "æ—¥å‹åŒ–", "1736": "å–¬å±±", "1737": "è‡ºé¹½",
    "1752": "å—å…‰", "1760": "å¯¶é½¡å¯ŒéŒ¦", "1762": "ä¸­åŒ–ç”Ÿ", "1773": "å‹ä¸€", "1776": "å±•å®‡",
    "1783": "å’Œåº·ç”Ÿ", "1786": "ç§‘å¦", "1789": "ç¥éš†", "1795": "ç¾æ™‚", "1802": "å°ç»",
    "1805": "å¯¶å¾ ", "1806": "å† è»", "1808": "æ½¤éš†", "1809": "ä¸­é‡‰", "1810": "å’Œæˆ",
    "1817": "å‡±æ’’è¡›", "1903": "å£«ç´™", "1904": "æ­£éš†", "1905": "è¯ç´™", "1906": "å¯¶éš†",
    "1907": "æ°¸è±é¤˜", "1909": "æ¦®æˆ", "2002": "ä¸­é‹¼", "2006": "æ±å’Œé‹¼éµ", "2007": "ç‡èˆˆ",
    "2008": "é«˜èˆˆæ˜Œ", "2009": "ç¬¬ä¸€éŠ…", "2010": "æ˜¥æº", "2012": "æ˜¥é›¨", "2013": "ä¸­é‹¼æ§‹",
    "2014": "ä¸­é´»", "2015": "è±èˆˆ", "2017": "å®˜ç”°é‹¼", "2020": "ç¾äº", "2022": "èšäº¨",
    "2023": "ç‡è¼", "2024": "å¿—è¯", "2025": "åƒèˆˆ", "2027": "å¤§æˆé‹¼", "2028": "å¨è‡´",
    "2029": "ç››é¤˜", "2030": "å½°æº", "2031": "æ–°å…‰é‹¼", "2032": "æ–°é‹¼", "2033": "ä½³å¤§",
    "2034": "å…å¼·", "2038": "æµ·å…‰", "2049": "ä¸ŠéŠ€", "2059": "å·æ¹–", "2062": "æ©‹æ¤¿",
    "2069": "é‹éŒ©", "2101": "å—æ¸¯", "2102": "æ³°è±", "2103": "å°æ©¡", "2104": "åœ‹éš›ä¸­æ©¡",
    "2105": "æ­£æ–°", "2106": "å»ºå¤§", "2107": "åšç”Ÿ", "2108": "å—å¸", "2109": "è¯è±",
    "2114": "é‘«æ°¸éŠ“", "2115": "å…­æš‰-KY", "2201": "è£•éš†", "2204": "ä¸­è¯", "2206": "ä¸‰é™½å·¥æ¥­",
    "2207": "å’Œæ³°è»Š", "2208": "å°èˆ¹", "2211": "é•·æ¦®é‹¼", "2227": "è£•æ—¥è»Š", "2228": "åŠéºŸ",
    "2231": "ç‚ºå‡", "2233": "å®‡éš†", "2236": "ç™¾é”-KY", "2239": "è‹±åˆ©-KY", "2241": "è‰¾å§†å‹’",
    "2243": "å®æ—­-KY", "2247": "æ±å¾·æ°¸æ¥­", "2248": "è¯å‹-KY", "2250": "IKKA-KY", "2301": "å…‰å¯¶ç§‘",
    "2302": "éº—æ­£", "2303": "è¯é›»", "2305": "å…¨å‹", "2308": "å°é”é›»", "2312": "é‡‘å¯¶",
    "2313": "è¯é€š", "2314": "å°æš", "2316": "æ¥ æ¢“é›»", "2317": "é´»æµ·", "2321": "æ±è¨Š",
    "2323": "ä¸­ç’°", "2324": "ä»å¯¶", "2327": "åœ‹å·¨*", "2328": "å»£å®‡", "2329": "è¯æ³°",
    "2330": "å°ç©é›»", "2331": "ç²¾è‹±", "2332": "å‹è¨Š", "2337": "æ—ºå®", "2338": "å…‰ç½©",
    "2340": "å°äº", "2342": "èŒ‚çŸ½", "2344": "è¯é‚¦é›»", "2345": "æ™ºé‚¦", "2347": "è¯å¼·",
    "2348": "æµ·æ‚…", "2349": "éŒ¸å¾·", "2351": "é †å¾·", "2352": "ä½³ä¸–é”", "2353": "å®ç¢",
    "2354": "é´»æº–", "2355": "æ•¬éµ¬", "2356": "è‹±æ¥­é”", "2357": "è¯ç¢©", "2359": "æ‰€ç¾…é–€",
    "2360": "è‡´èŒ‚", "2362": "è—å¤©", "2363": "çŸ½çµ±", "2364": "å€«é£›", "2365": "æ˜†ç›ˆ"
}


def push_image(original_url: str, preview_url: str):
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {
        "to": LINE_USER_ID,
        "messages": [{
            "type": "image",
            "originalContentUrl": original_url,
            "previewImageUrl": preview_url
        }]
    }
    r = requests.post(url, headers=headers, json=body, timeout=30)
    r.raise_for_status()

def line_push_text(msg: str):
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {"to": LINE_USER_ID, "messages": [{"type": "text", "text": msg}]}
    r = requests.post(url, headers=headers, json=body, timeout=30)
    r.raise_for_status()


def get_drive_service():
    """å»ºç«‹ Google Drive API æœå‹™ï¼ˆä½¿ç”¨ OAuth 2.0ï¼‰"""
    if not OAUTH_CREDENTIALS:
        raise ValueError("æœªè¨­å®š OAUTH ç’°å¢ƒè®Šæ•¸ï¼Œç„¡æ³•é€²è¡Œ Google Drive èªè­‰")

    try:
        print("ğŸ” Google Drive OAuth 2.0 èªè­‰...")
        oauth_data = json.loads(OAUTH_CREDENTIALS)

        creds = Credentials(
            token=oauth_data.get('token'),
            refresh_token=oauth_data.get('refresh_token'),
            token_uri=oauth_data.get('token_uri'),
            client_id=oauth_data.get('client_id'),
            client_secret=oauth_data.get('client_secret'),
            scopes=SCOPES
        )

        if creds.expired and creds.refresh_token:
            print("ğŸ”„ é‡æ–°æ•´ç† Google Drive æˆæ¬Š...")
            creds.refresh(Request())

        service = build('drive', 'v3', credentials=creds)
        print("âœ… Google Drive OAuth èªè­‰æˆåŠŸ")
        return service
    except Exception as e:
        print(f"âŒ OAuth èªè­‰å¤±æ•—: {e}")
        raise


def find_folder(service, folder_name, parent_id=None):
    """å°‹æ‰¾æŒ‡å®šåç¨±çš„è³‡æ–™å¤¾"""
    if not service:
        return None

    try:
        query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false"
        if parent_id:
            query += f" and '{parent_id}' in parents"

        results = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
        items = results.get('files', [])

        if items:
            return items[0]['id']
        return None
    except Exception as e:
        print(f"âŒ å°‹æ‰¾è³‡æ–™å¤¾å¤±æ•—: {e}")
        return None


def create_folder(service, folder_name, parent_id=None):
    """å»ºç«‹è³‡æ–™å¤¾"""
    if not service:
        return None

    try:
        file_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        if parent_id:
            file_metadata['parents'] = [parent_id]

        folder = service.files().create(body=file_metadata, fields='id').execute()
        print(f"âœ… å·²å»ºç«‹è³‡æ–™å¤¾: {folder_name}")
        return folder.get('id')
    except Exception as e:
        print(f"âŒ å»ºç«‹è³‡æ–™å¤¾å¤±æ•—: {e}")
        return None


def download_file_from_drive(service, file_name, folder_id, local_path):
    """å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆ"""
    if not service:
        logger.warning("Google Drive æœå‹™ä¸å¯ç”¨")
        return False

    try:
        # å°‹æ‰¾æª”æ¡ˆ
        logger.debug(f"æœå°‹æª”æ¡ˆ: {file_name} åœ¨è³‡æ–™å¤¾: {folder_id}")
        query = f"name='{file_name}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields='files(id, name, size, modifiedTime)').execute()
        items = results.get('files', [])

        if not items:
            logger.info(f"ğŸ“ Google Drive ä¸­æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_name}")
            return False

        file_id = items[0]['id']
        file_size = int(items[0].get('size', 0)) / 1024 / 1024  # MB
        logger.debug(f"æ‰¾åˆ°æª”æ¡ˆ - ID: {file_id}, å¤§å°: {file_size:.2f} MB")

        # ä¸‹è¼‰æª”æ¡ˆ
        logger.info(f"é–‹å§‹ä¸‹è¼‰: {file_name}")
        request = service.files().get_media(fileId=file_id)
        file_buffer = io.BytesIO()
        downloader = MediaIoBaseDownload(file_buffer, request)

        done = False
        while done is False:
            status, done = downloader.next_chunk()
            if DEBUG_MODE and status:
                logger.debug(f"ä¸‹è¼‰é€²åº¦: {int(status.progress() * 100)}%")

        # å¯«å…¥æœ¬åœ°æª”æ¡ˆ
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, 'wb') as f:
            f.write(file_buffer.getvalue())

        logger.info(f"âœ… å·²å¾ Google Drive ä¸‹è¼‰: {file_name} -> {local_path}")
        logger.info(f"ğŸ“¥ ä¸‹è¼‰å®Œæˆ - æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
        return True

    except Exception as e:
        logger.error(f"âŒ ä¸‹è¼‰æª”æ¡ˆå¤±æ•—: {e}")
        if DEBUG_MODE:
            logger.debug(f"è©³ç´°éŒ¯èª¤: {str(e)}", exc_info=True)
        return False


def upload_file_to_drive(service, local_path, file_name, folder_id):
    """ä¸Šå‚³æª”æ¡ˆåˆ° Google Drive"""
    if not service:
        logger.warning("Google Drive æœå‹™ä¸å¯ç”¨")
        return False

    try:
        # å–å¾—æœ¬åœ°æª”æ¡ˆå¤§å°
        file_size = os.path.getsize(local_path) / 1024 / 1024  # MB
        logger.debug(f"æº–å‚™ä¸Šå‚³æª”æ¡ˆ: {file_name}, å¤§å°: {file_size:.2f} MB")

        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
        logger.debug(f"æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨: {file_name}")
        query = f"name='{file_name}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields='files(id, name)').execute()
        items = results.get('files', [])

        file_metadata = {'name': file_name, 'parents': [folder_id]}
        media = MediaFileUpload(local_path, resumable=True)

        if items:
            # æ›´æ–°ç¾æœ‰æª”æ¡ˆ
            file_id = items[0]['id']
            logger.info(f"æ›´æ–°ç¾æœ‰æª”æ¡ˆ: {file_name} (ID: {file_id})")
            file = service.files().update(fileId=file_id, media_body=media).execute()
            logger.info(f"âœ… å·²æ›´æ–° Google Drive æª”æ¡ˆ: {file_name}")
            logger.info(f"ğŸ“¤ æ›´æ–°å®Œæˆ - æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
        else:
            # å»ºç«‹æ–°æª”æ¡ˆ
            logger.info(f"å»ºç«‹æ–°æª”æ¡ˆ: {file_name}")
            file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()
            logger.info(f"âœ… å·²ä¸Šå‚³æ–°æª”æ¡ˆåˆ° Google Drive: {file_name}")
            logger.info(f"ğŸ“¤ ä¸Šå‚³å®Œæˆ - æª”æ¡ˆID: {file.get('id')}, å¤§å°: {file_size:.2f} MB")

        return True

    except Exception as e:
        logger.error(f"âŒ ä¸Šå‚³æª”æ¡ˆå¤±æ•—: {e}")
        if DEBUG_MODE:
            logger.debug(f"æª”æ¡ˆè·¯å¾‘: {local_path}")
            logger.debug(f"ç›®æ¨™è³‡æ–™å¤¾: {folder_id}")
            logger.debug(f"è©³ç´°éŒ¯èª¤: {str(e)}", exc_info=True)
        return False


def upload_to_google_drive(file_path: str, filename: str, folder_id: str, mimetype: str = 'image/png') -> str:
    """ä¸Šå‚³æª”æ¡ˆåˆ° Google Drive ä¸¦è¿”å›åˆ†äº«é€£çµ"""
    logger.info(f"ğŸ“¤ ä¸Šå‚³æª”æ¡ˆåˆ° Google Drive: {filename}")

    try:
        service = get_drive_service()

        # å–å¾—æª”æ¡ˆå¤§å°
        file_size = os.path.getsize(file_path) / 1024 / 1024  # MB
        logger.debug(f"æª”æ¡ˆå¤§å°: {file_size:.2f} MB, é¡å‹: {mimetype}")

        file_metadata = {
            'name': filename,
            'parents': [folder_id]
        }

        media = MediaFileUpload(file_path, mimetype=mimetype, resumable=True)
        file = service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id, webViewLink'
        ).execute()

        file_id = file.get('id')
        logger.debug(f"æª”æ¡ˆå·²å»ºç«‹, ID: {file_id}")

        # è¨­å®šåˆ†äº«æ¬Šé™
        logger.debug("è¨­å®šæª”æ¡ˆç‚ºå…¬é–‹åˆ†äº«")
        service.permissions().create(
            fileId=file_id,
            body={'type': 'anyone', 'role': 'reader'}
        ).execute()

        web_link = file.get('webViewLink')
        logger.info(f"âœ… æª”æ¡ˆä¸Šå‚³æˆåŠŸ: {filename}")
        logger.info(f"ğŸ”— åˆ†äº«é€£çµ: {web_link}")

        return web_link
    except Exception as e:
        logger.error(f"âŒ Google Drive ä¸Šå‚³å¤±æ•—: {e}")
        if DEBUG_MODE:
            logger.debug(f"æª”æ¡ˆè·¯å¾‘: {file_path}")
            logger.debug(f"ç›®æ¨™è³‡æ–™å¤¾: {folder_id}")
            logger.debug(f"è©³ç´°éŒ¯èª¤: {str(e)}", exc_info=True)
        return None


def upload_text_to_google_drive(text_content: str, filename: str, folder_id: str) -> str:
    """å°‡æ–‡å­—å…§å®¹å­˜ç‚º txt ä¸¦ä¸Šå‚³åˆ° Google Drive"""
    logger.info(f"ğŸ“„ æº–å‚™ä¸Šå‚³æ–‡å­—æª”æ¡ˆ: {filename}")

    try:
        # å»ºç«‹æš«å­˜æª”æ¡ˆ
        temp_file = tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix='.txt')
        temp_file.write(text_content)
        temp_file.close()

        logger.debug(f"æš«å­˜æª”æ¡ˆå»ºç«‹: {temp_file.name}")
        logger.debug(f"æ–‡å­—å…§å®¹å¤§å°: {len(text_content)} å­—å…ƒ")

        # ä¸Šå‚³åˆ° Google Drive
        result = upload_to_google_drive(temp_file.name, filename, folder_id, mimetype='text/plain')

        # åˆªé™¤æš«å­˜æª”æ¡ˆ
        os.unlink(temp_file.name)
        logger.debug("æš«å­˜æª”æ¡ˆå·²åˆªé™¤")

        if result:
            logger.info(f"âœ… æ–‡å­—æª”æ¡ˆä¸Šå‚³æˆåŠŸ: {filename}")
        else:
            logger.error(f"âŒ æ–‡å­—æª”æ¡ˆä¸Šå‚³å¤±æ•—: {filename}")

        return result
    except Exception as e:
        logger.error(f"âŒ ä¸Šå‚³æ–‡å­—æª”å¤±æ•—: {e}")
        if DEBUG_MODE:
            logger.debug(f"æª”æ¡ˆåç¨±: {filename}")
            logger.debug(f"æ–‡å­—é•·åº¦: {len(text_content)}")
            logger.debug(f"è©³ç´°éŒ¯èª¤: {str(e)}", exc_info=True)
        return None


def setup_google_drive_folders(service):
    """è¨­å®š Google Drive è³‡æ–™å¤¾çµæ§‹"""
    if not service:
        return None

    try:
        # å¦‚æœæœ‰ç›´æ¥æŒ‡å®šè³‡æ–™å¤¾ IDï¼Œå„ªå…ˆä½¿ç”¨ï¼ˆæ”¯æ´å…©ç¨®è®Šæ•¸åç¨±ï¼‰
        folder_id = GOOGLE_DRIVE_FOLDER_ID or GDRIVE_FOLDER_ID
        if folder_id:
            print(f"âœ… ä½¿ç”¨æŒ‡å®šçš„ Google Drive è³‡æ–™å¤¾ ID: {folder_id}")
            main_folder_id = folder_id
        else:
            # å°‹æ‰¾æˆ–å»ºç«‹ä¸»è³‡æ–™å¤¾ stocks-autobot-data
            print(f"ğŸ” æœå°‹è³‡æ–™å¤¾: {GDRIVE_FOLDER_NAME}")
            main_folder_id = find_folder(service, GDRIVE_FOLDER_NAME)
            if not main_folder_id:
                main_folder_id = create_folder(service, GDRIVE_FOLDER_NAME)

            if not main_folder_id:
                print("âŒ ç„¡æ³•å»ºç«‹ä¸»è³‡æ–™å¤¾")
                return None

        # å°‹æ‰¾æˆ–å»ºç«‹ data å­è³‡æ–™å¤¾
        data_folder_id = find_folder(service, GDRIVE_DATA_FOLDER, main_folder_id)
        if not data_folder_id:
            data_folder_id = create_folder(service, GDRIVE_DATA_FOLDER, main_folder_id)

        if data_folder_id:
            print(f"âœ… Google Drive è³‡æ–™å¤¾å·²æº–å‚™å°±ç·’: {GDRIVE_DATA_FOLDER}")

        return data_folder_id

    except Exception as e:
        print(f"âŒ è¨­å®š Google Drive è³‡æ–™å¤¾å¤±æ•—: {e}")
        return None


def sync_database_from_drive(service):
    """å¾ Google Drive åŒæ­¥è³‡æ–™åº«åˆ°æœ¬åœ°"""
    logger.info("ğŸ“¥ é–‹å§‹å¾ Google Drive åŒæ­¥è³‡æ–™åº«")

    if not service:
        logger.warning("âš ï¸  è·³é Google Drive ä¸‹è¼‰ï¼ˆService ä¸å¯ç”¨ï¼‰")
        return False

    try:
        logger.debug("è¨­å®š Google Drive è³‡æ–™å¤¾çµæ§‹")
        data_folder_id = setup_google_drive_folders(service)
        if not data_folder_id:
            logger.error("ç„¡æ³•å–å¾— Google Drive data è³‡æ–™å¤¾ ID")
            return False

        logger.debug(f"Data è³‡æ–™å¤¾ ID: {data_folder_id}")

        # ä¸‹è¼‰ taiex.sqlite
        logger.info("ä¸‹è¼‰ taiex.sqlite æª”æ¡ˆ")
        success = download_file_from_drive(service, "taiex.sqlite", data_folder_id, DB_PATH)

        if success:
            logger.info("âœ… è³‡æ–™åº«å¾ Google Drive åŒæ­¥æˆåŠŸ")
            if DEBUG_MODE:
                import os
                file_size = os.path.getsize(DB_PATH) / 1024 / 1024  # MB
                logger.debug(f"è³‡æ–™åº«æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
        else:
            logger.warning("âš ï¸  è³‡æ–™åº«åŒæ­¥å¤±æ•—æˆ–æª”æ¡ˆä¸å­˜åœ¨")

        return success

    except Exception as e:
        logger.error(f"âŒ å¾ Google Drive åŒæ­¥è³‡æ–™åº«å¤±æ•—: {e}")
        if DEBUG_MODE:
            logger.debug(f"è©³ç´°éŒ¯èª¤: {str(e)}", exc_info=True)
        return False


def sync_database_to_drive(service):
    """ä¸Šå‚³æœ¬åœ°è³‡æ–™åº«åˆ° Google Drive"""
    logger.info("ğŸ“¤ é–‹å§‹ä¸Šå‚³è³‡æ–™åº«åˆ° Google Drive")

    if not service:
        logger.warning("âš ï¸  è·³é Google Drive ä¸Šå‚³ï¼ˆService ä¸å¯ç”¨ï¼‰")
        return False

    try:
        if not os.path.exists(DB_PATH):
            logger.warning(f"âš ï¸  æœ¬åœ°è³‡æ–™åº«ä¸å­˜åœ¨: {DB_PATH}")
            return False

        # å–å¾—æª”æ¡ˆå¤§å°
        file_size = os.path.getsize(DB_PATH) / 1024 / 1024  # MB
        logger.info(f"è³‡æ–™åº«æª”æ¡ˆå¤§å°: {file_size:.2f} MB")

        logger.debug("è¨­å®š Google Drive è³‡æ–™å¤¾çµæ§‹")
        data_folder_id = setup_google_drive_folders(service)
        if not data_folder_id:
            logger.error("ç„¡æ³•å–å¾— Google Drive data è³‡æ–™å¤¾ ID")
            return False

        logger.debug(f"Data è³‡æ–™å¤¾ ID: {data_folder_id}")

        # ä¸Šå‚³ taiex.sqlite
        logger.info("é–‹å§‹ä¸Šå‚³ taiex.sqlite åˆ° Google Drive")
        success = upload_file_to_drive(service, DB_PATH, "taiex.sqlite", data_folder_id)

        if success:
            logger.info("âœ… è³‡æ–™åº«æˆåŠŸä¸Šå‚³åˆ° Google Drive")
            logger.info(f"ğŸ“Š ä¸Šå‚³å®Œæˆ - æª”æ¡ˆ: taiex.sqlite, å¤§å°: {file_size:.2f} MB")
        else:
            logger.error("âŒ è³‡æ–™åº«ä¸Šå‚³å¤±æ•—")

        return success

    except Exception as e:
        logger.error(f"âŒ ä¸Šå‚³è³‡æ–™åº«åˆ° Google Drive å¤±æ•—: {e}")
        if DEBUG_MODE:
            logger.debug(f"è©³ç´°éŒ¯èª¤: {str(e)}", exc_info=True)
        return False


def ensure_db():
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS prices(
                code TEXT,
                date TEXT,
                open REAL, high REAL, low REAL, close REAL,
                volume INTEGER,
                PRIMARY KEY(code, date)
            )
            """
        )
        conn.commit()


def get_existing_data_range() -> dict:
    if not os.path.exists(DB_PATH):
        return {}
    with sqlite3.connect(DB_PATH) as conn:
        cursor = conn.execute(
            "SELECT code, MIN(date) as min_date, MAX(date) as max_date FROM prices GROUP BY code"
        )
        result = {}
        for row in cursor:
            result[row[0]] = {"min": row[1], "max": row[2]}
    return result


def fetch_prices_yf(codes, lookback_days=120) -> pd.DataFrame:
    existing = get_existing_data_range()
    target_start = (datetime.utcnow() - timedelta(days=lookback_days * 2)).date().isoformat()

    codes_to_fetch = []
    for c in codes:
        c = c.strip()
        if not c:
            continue
        if c not in existing:
            codes_to_fetch.append(c)
            print(f"{c}: ç„¡æ­·å²è³‡æ–™ï¼Œéœ€ä¸‹è¼‰")
        else:
            max_date = existing[c]["max"]
            if max_date < (datetime.utcnow() - timedelta(days=2)).date().isoformat():
                codes_to_fetch.append(c)
                print(f"{c}: è³‡æ–™éèˆŠ (æœ€æ–°: {max_date})ï¼Œéœ€æ›´æ–°")
            else:
                print(f"{c}: è³‡æ–™å·²æ˜¯æœ€æ–° (æœ€æ–°: {max_date})")

    if not codes_to_fetch:
        print("æ‰€æœ‰è‚¡ç¥¨è³‡æ–™éƒ½å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€ä¸‹è¼‰")
        return pd.DataFrame()

    tickers = [f"{c}.TW" for c in codes_to_fetch]
    print(f"\né–‹å§‹ä¸‹è¼‰ {len(codes_to_fetch)} æ”¯è‚¡ç¥¨")
    print(f"æœŸé–“: {target_start} ~ ä»Šæ—¥")

    df = yf.download(
        tickers=" ".join(tickers),
        start=target_start,
        interval="1d",
        group_by="ticker",
        auto_adjust=False,
        progress=False,
    )
    out = []
    for c in codes_to_fetch:
        t = f"{c}.TW"
        if isinstance(df, pd.DataFrame) and t in df:
            tmp = df[t].reset_index().rename(columns=str.lower)
            if "date" in tmp.columns:
                tmp["date"] = pd.to_datetime(tmp["date"]).dt.tz_localize(None)
            tmp["code"] = c
            out.append(tmp[["code", "date", "open", "high", "low", "close", "volume"]])
    result = pd.concat(out, ignore_index=True) if out else pd.DataFrame()
    print(f"æˆåŠŸä¸‹è¼‰ {len(result)} ç­†æ•¸æ“š")
    return result


def upsert_prices(df: pd.DataFrame):
    if df.empty:
        return
    df = df.copy()
    df["date"] = pd.to_datetime(df["date"]).dt.date.astype(str)
    with sqlite3.connect(DB_PATH) as conn:
        df.to_sql("_prices_in", conn, if_exists="replace", index=False)
        conn.execute("DELETE FROM prices WHERE (code, date) IN (SELECT code, date FROM _prices_in)")
        conn.execute(
            """
            INSERT INTO prices(code, date, open, high, low, close, volume)
            SELECT code, date, open, high, low, close, volume FROM _prices_in
            """
        )
        conn.execute("DROP TABLE _prices_in")
        conn.commit()
    print(f"æ•¸æ“šå·²å­˜å…¥è³‡æ–™åº«: {DB_PATH}")


def load_recent_prices(days=120) -> pd.DataFrame:
    with sqlite3.connect(DB_PATH) as conn:
        df = pd.read_sql_query(
            "SELECT code, date, open, high, low, close, volume FROM prices",
            conn,
            parse_dates=["date"],
        )
    cutoff = datetime.utcnow() - timedelta(days=days)
    return df[df["date"] >= cutoff]


def pick_stocks(prices: pd.DataFrame, top_k=30) -> pd.DataFrame:
    if prices.empty:
        return pd.DataFrame()
    prices = prices.sort_values(["code", "date"])

    def add_feat(g):
        g = g.copy()
        g["ma20"] = g["close"].rolling(20, min_periods=20).mean()
        return g

    feat = prices.groupby("code", group_keys=False).apply(add_feat)

    results = []
    for code, group in feat.groupby("code"):
        group = group.sort_values("date")
        if len(group) < 5:
            continue

        last_5 = group.tail(5)
        if last_5["ma20"].isna().any():
            continue

        open_above = (last_5["open"] > last_5["ma20"]).all()
        close_above = (last_5["close"] > last_5["ma20"]).all()

        if not (open_above and close_above):
            continue

        ma20_values = last_5["ma20"].values
        ma20_slope = (ma20_values[-1] - ma20_values[0]) / 4

        if ma20_slope >= 1:
            continue
        price_std = last_5["close"].std()
        price_mean = last_5["close"].mean()
        volatility_pct = (price_std / price_mean * 100) if price_mean > 0 else 999
        if volatility_pct > 3.0:
            continue

        max_distance_allowed = max(2.0, volatility_pct * 1.5)

        min_price = last_5[["open", "close"]].min(axis=1)
        distance_pct = ((min_price - last_5["ma20"]) / last_5["ma20"] * 100)
        avg_distance = distance_pct.mean()

        if avg_distance > max_distance_allowed:
            continue

        avg_price = (last_5["open"] + last_5["close"]) / 2
        avg_ma20_distance = abs(avg_price - last_5["ma20"]).mean()

        latest = last_5.iloc[-1]
        is_lowest_close = latest["close"] == last_5["close"].min()

        results.append({
            "code": code,
            "close": latest["close"],
            "ma20": latest["ma20"],
            "distance": avg_distance,
            "volatility": volatility_pct,
            "ma20_slope": ma20_slope,
            "max_distance": max_distance_allowed,
            "volume": latest["volume"],
            "avg_ma20_distance": avg_ma20_distance,
            "is_lowest_close": is_lowest_close
        })

    if not results:
        return pd.DataFrame()

    result_df = pd.DataFrame(results)

    group1 = result_df[(result_df["ma20_slope"] >= 0.5) & (result_df["ma20_slope"] < 1)]
    group2 = result_df[result_df["ma20_slope"] < 0.5]

    if len(group1) > 6:
        group1_filtered = group1[group1["is_lowest_close"] == False]
        if len(group1_filtered) > 6:
            group1 = group1_filtered.nsmallest(6, "avg_ma20_distance")
        elif len(group1_filtered) > 0:
            group1 = group1_filtered
        else:
            group1 = group1.nsmallest(6, "avg_ma20_distance")

    if len(group2) > 6:
        group2_filtered = group2[group2["is_lowest_close"] == False]
        if len(group2_filtered) > 6:
            group2 = group2_filtered.nsmallest(6, "avg_ma20_distance")
        elif len(group2_filtered) > 0:
            group2 = group2_filtered
        else:
            group2 = group2.nsmallest(6, "avg_ma20_distance")

    final_result = pd.concat([group1, group2], ignore_index=True)
    return final_result


def plot_candlestick(ax, stock_data):
    """åœ¨æŒ‡å®šçš„ ax ä¸Šç¹ªè£½ K æ£’åœ–"""
    stock_data = stock_data.copy().reset_index(drop=True)

    for idx, row in stock_data.iterrows():
        date_num = idx
        open_price = row['open']
        high_price = row['high']
        low_price = row['low']
        close_price = row['close']

        color = '#FF4444' if close_price >= open_price else '#00AA00'

        ax.plot([date_num, date_num], [low_price, high_price], color=color, linewidth=0.8)

        body_height = abs(close_price - open_price)
        body_bottom = min(open_price, close_price)
        ax.bar(date_num, body_height, bottom=body_bottom, color=color, width=0.6, alpha=0.8)


def plot_stock_charts(codes: list, prices: pd.DataFrame) -> str:
    """ç¹ªè£½æœ€å¤š 6 æ”¯è‚¡ç¥¨çš„ K æ£’åœ–ï¼ˆ2x3 å­åœ–ï¼‰"""
    logger.debug(f'é–‹å§‹ç¹ªè£½åœ–è¡¨ï¼Œè‚¡ç¥¨ä»£ç¢¼: {codes}')
    codes = codes[:6]
    n_stocks = len(codes)
    if n_stocks == 0:
        logger.warning("æ²’æœ‰è‚¡ç¥¨ä»£ç¢¼éœ€è¦ç¹ªè£½")
        return None

    # è¨­å®šå­—é«”å„ªå…ˆç´šï¼šWindowså­—é«” -> Linuxå­—é«” -> é€šç”¨å­—é«”
    fonts = ['Microsoft JhengHei', 'SimHei', 'WenQuanYi Zen Hei', 'WenQuanYi Micro Hei', 'DejaVu Sans', 'Arial']
    plt.rcParams['font.sans-serif'] = fonts
    plt.rcParams['axes.unicode_minus'] = False

    if DEBUG_MODE:
        logger.debug(f"matplotlib å¾Œç«¯: {matplotlib.get_backend()}")
        logger.debug(f"è¨­å®šå­—é«”é †åº: {fonts}")

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()

    for i, code in enumerate(codes):
        stock_data = prices[prices["code"] == code].sort_values("date").tail(90)

        if stock_data.empty or len(stock_data) < 20:
            axes[i].text(0.5, 0.5, f"{code} {STOCK_NAMES.get(code, '')}\næ•¸æ“šä¸è¶³",
                        ha='center', va='center', fontsize=14)
            axes[i].set_xticks([])
            axes[i].set_yticks([])
            continue

        stock_data = stock_data.copy()
        stock_data["ma20"] = stock_data["close"].rolling(20, min_periods=20).mean()

        ax = axes[i]
        plot_candlestick(ax, stock_data)

        valid_ma20 = stock_data[stock_data["ma20"].notna()]
        if not valid_ma20.empty:
            ma20_indices = valid_ma20.index - stock_data.index[0]
            ax.plot(ma20_indices, valid_ma20["ma20"], label="MA20",
                   linewidth=2, linestyle="--", alpha=0.7, color='#2E86DE')

        stock_name = STOCK_NAMES.get(code, code)
        ax.set_title(f"{code} {stock_name}", fontsize=14, fontweight='bold', pad=10)
        ax.legend(fontsize=9, loc='upper left')
        ax.grid(True, alpha=0.3, linestyle='--')

        date_labels = stock_data["date"].dt.strftime('%m/%d').tolist()
        step = max(1, len(date_labels) // 6)
        tick_positions = list(range(0, len(date_labels), step))
        tick_labels = [date_labels[i] for i in tick_positions]
        ax.set_xticks(tick_positions)
        ax.set_xticklabels(tick_labels, rotation=45, fontsize=9)
        ax.tick_params(axis='y', labelsize=9)

    for i in range(n_stocks, 6):
        fig.delaxes(axes[i])

    plt.tight_layout()

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
    plt.savefig(temp_file.name, dpi=100, bbox_inches='tight')
    plt.close()

    return temp_file.name


def upload_to_telegraph(image_path: str) -> str:
    """ä½¿ç”¨ Telegraph ä¸Šå‚³åœ–ç‰‡ï¼ˆç„¡éœ€ API keyï¼‰"""
    try:
        with open(image_path, 'rb') as f:
            response = requests.post(
                'https://telegra.ph/upload',
                files={'file': ('image.png', f, 'image/png')},
                timeout=30
            )

        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                path = result[0].get('src')
                if path:
                    return f"https://telegra.ph{path}"

        print(f"Telegraph ä¸Šå‚³å¤±æ•—: {response.status_code} - {response.text[:200]}")
    except Exception as e:
        print(f"Telegraph ä¸Šå‚³ç•°å¸¸: {e}")
    return None


def upload_to_catbox(image_path: str) -> str:
    """ä½¿ç”¨ Catbox ä¸Šå‚³åœ–ç‰‡ï¼ˆç„¡éœ€ API keyï¼Œå‚™ç”¨ï¼‰"""
    try:
        with open(image_path, 'rb') as f:
            response = requests.post(
                'https://catbox.moe/user/api.php',
                data={'reqtype': 'fileupload'},
                files={'fileToUpload': f},
                timeout=30
            )

        if response.status_code == 200:
            url = response.text.strip()
            if url.startswith('https://'):
                return url

        print(f"Catbox ä¸Šå‚³å¤±æ•—: {response.status_code} - {response.text[:200]}")
    except Exception as e:
        print(f"Catbox ä¸Šå‚³ç•°å¸¸: {e}")
    return None


def upload_image(image_path: str) -> str:
    """å˜—è©¦å¤šå€‹åœ–åºŠä¸Šå‚³ï¼Œè¿”å›ç¬¬ä¸€å€‹æˆåŠŸçš„ URL"""
    print(f"å˜—è©¦ä¸Šå‚³åœ–ç‰‡: {image_path}")

    url = upload_to_telegraph(image_path)
    if url:
        print(f"âœ… Telegraph ä¸Šå‚³æˆåŠŸ: {url}")
        return url

    print("â†’ å˜—è©¦å‚™ç”¨åœ–åºŠ Catbox...")
    url = upload_to_catbox(image_path)
    if url:
        print(f"âœ… Catbox ä¸Šå‚³æˆåŠŸ: {url}")
        return url

    print("âŒ æ‰€æœ‰åœ–åºŠä¸Šå‚³å¤±æ•—")
    return None


def main():
    logger.info("=" * 50)
    logger.info("ğŸš€ å°è‚¡æ¨è–¦æ©Ÿå™¨äººè‡ªå‹•åŸ·è¡Œ")
    logger.info("=" * 50)
    start_time = datetime.now()

    try:
        logger.info("\nğŸ“Œ æ­¥é©Ÿ 1: è¨­å®š Google Drive é€£ç·š")
        drive_service = get_drive_service()

        logger.info("\nğŸ“Œ æ­¥é©Ÿ 2: å¾ Google Drive åŒæ­¥è³‡æ–™åº«")
        sync_database_from_drive(drive_service)

        logger.info("\nğŸ“Œ æ­¥é©Ÿ 3: å»ºç«‹è³‡æ–™åº«")
        ensure_db()
        logger.debug(f"è³‡æ–™åº«è·¯å¾‘: {DB_PATH}")

        logger.info("\nğŸ“Œ æ­¥é©Ÿ 4: æª¢æŸ¥ä¸¦ä¸‹è¼‰éœ€è¦çš„æ•¸æ“š")
        df_new = fetch_prices_yf(CODES, lookback_days=120)
        data_updated = False
        if not df_new.empty:
            upsert_prices(df_new)
            data_updated = True
            logger.info("âœ… è³‡æ–™åº«å·²æ›´æ–°")
        else:
            logger.info("â„¹ï¸  ç„¡éœ€æ›´æ–°è³‡æ–™åº«")

        logger.info("\nğŸ“Œ æ­¥é©Ÿ 5: è¼‰å…¥æ•¸æ“šä¸¦ç¯©é¸è‚¡ç¥¨")
        hist = load_recent_prices(days=120)
        picks = pick_stocks(hist, top_k=PICKS_TOP_K)
        logger.debug(f"è¼‰å…¥ {len(hist)} ç­†æ­·å²è³‡æ–™")
        logger.debug(f"ç¯©é¸å‡º {len(picks)} æ”¯ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")

        logger.info("\nğŸ“Œ æ­¥é©Ÿ 6: å°‡è‚¡ç¥¨åˆ†çµ„")
        today_tpe = datetime.now(timezone(timedelta(hours=8))).date()
        today_weekday = today_tpe.weekday()  # 0=é€±ä¸€, 6=é€±æ—¥

        if picks.empty:
            group1 = pd.DataFrame()
            group2 = pd.DataFrame()
        else:
            group1 = picks[(picks["ma20_slope"] >= 0.5) & (picks["ma20_slope"] < 1)]
            group2 = picks[picks["ma20_slope"] < 0.5]

        logger.info(f"ğŸ“ˆ å¥½åƒè »å¼·çš„ï¼ˆæ–œç‡ 0.5-1ï¼‰ï¼š{len(group1)} æ”¯")
        logger.info(f"ğŸ“Š æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ï¼ˆæ–œç‡ < 0.5ï¼‰ï¼š{len(group2)} æ”¯")

        logger.info("\nğŸ“Œ æ­¥é©Ÿ 7: ç™¼é€ LINE è¨Šæ¯")

        # æª¢æŸ¥æ˜¯å¦ç‚ºé€±æœ«ï¼ˆé€±å…­=5, é€±æ—¥=6ï¼‰
        if today_weekday >= 5:
            weekday_names = ["é€±ä¸€", "é€±äºŒ", "é€±ä¸‰", "é€±å››", "é€±äº”", "é€±å…­", "é€±æ—¥"]
            logger.info(f"ğŸ—“ï¸  ä»Šæ—¥ç‚º{weekday_names[today_weekday]} ({today_tpe})ï¼Œè‚¡å¸‚ä¼‘å¸‚ï¼Œè·³éç™¼é€è¨Šæ¯")
            logger.info("ğŸ“´ é€±æœ«ä¸ç™¼é€è‚¡ç¥¨æ¨è–¦è¨Šæ¯")
        else:
            # å¹³æ—¥ç™¼é€è¨Šæ¯
            if group1.empty and group2.empty:
                msg = f"ğŸ“‰ {today_tpe}\nä»Šæ—¥ç„¡ç¬¦åˆæ¢ä»¶ä¹‹å°è‚¡æ¨è–¦ã€‚"
                logger.info(f"å°‡ç™¼é€çš„è¨Šæ¯:\n{msg}")
                try:
                    line_push_text(msg)
                    logger.info("âœ… LINE è¨Šæ¯ç™¼é€æˆåŠŸï¼")
                except Exception as e:
                    logger.error(f"âŒ LINE è¨Šæ¯ç™¼é€å¤±æ•—: {e}")
            else:
                if not group1.empty:
                    logger.info("\nè™•ç†ã€Œå¥½åƒè »å¼·çš„ã€çµ„...")
                    lines = [f"ğŸ’ª å¥½åƒè »å¼·çš„ ({today_tpe})"]
                    lines.append("ä»¥ä¸‹è‚¡ç¥¨å¯ä»¥åƒè€ƒï¼š\n")
                    for i, r in group1.iterrows():
                        stock_name = STOCK_NAMES.get(r.code, r.code)
                        lines.append(f"{r.code} {stock_name}")
                    msg1 = "\n".join(lines)
                    logger.info(f"è¨Šæ¯:\n{msg1}")

                    try:
                        line_push_text(msg1)
                        logger.info("âœ… å¥½åƒè »å¼·çš„çµ„è¨Šæ¯ç™¼é€æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"âŒ å¥½åƒè »å¼·çš„çµ„è¨Šæ¯ç™¼é€å¤±æ•—: {e}")

                logger.info("\nç”Ÿæˆä¸¦ç™¼é€ã€Œå¥½åƒè »å¼·çš„ã€çµ„åœ–ç‰‡")
                group1_codes = group1["code"].tolist()
                for batch_num in range(0, len(group1_codes), 6):
                    batch_codes = group1_codes[batch_num:batch_num + 6]
                    batch_display = ", ".join(batch_codes)
                    logger.info(f"æ­£åœ¨è™•ç†å¥½åƒè »å¼·çš„ç¬¬ {batch_num//6 + 1} çµ„: {batch_display}")

                    chart_path = plot_stock_charts(batch_codes, hist)
                    if chart_path:
                        img_url = upload_image(chart_path)
                        if img_url:
                            try:
                                push_image(img_url, img_url)
                                logger.info(f"âœ… åœ–è¡¨å·²ç™¼é€åˆ° LINE")
                            except Exception as e:
                                logger.error(f"âŒ LINE ç™¼é€å¤±æ•—: {e}")
                            os.unlink(chart_path)
                        else:
                            logger.warning(f"âŒ åœ–åºŠä¸Šå‚³å¤±æ•—")
                    else:
                        logger.warning(f"âŒ åœ–è¡¨ç”Ÿæˆå¤±æ•—")

                if not group2.empty:
                    logger.info("\nè™•ç†ã€Œæœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ã€çµ„...")
                    lines = [f"ğŸ‘€ æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ ({today_tpe})"]
                    lines.append("ä»¥ä¸‹è‚¡ç¥¨å¯ä»¥åƒè€ƒï¼š\n")
                    for i, r in group2.iterrows():
                        stock_name = STOCK_NAMES.get(r.code, r.code)
                        lines.append(f"{r.code} {stock_name}")
                    msg2 = "\n".join(lines)
                    logger.info(f"è¨Šæ¯:\n{msg2}")

                    try:
                        line_push_text(msg2)
                        logger.info("âœ… æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹çµ„è¨Šæ¯ç™¼é€æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"âŒ æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹çµ„è¨Šæ¯ç™¼é€å¤±æ•—: {e}")

                    logger.info("\nç”Ÿæˆä¸¦ç™¼é€ã€Œæœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ã€çµ„åœ–ç‰‡")
                    group2_codes = group2["code"].tolist()
                    for batch_num in range(0, len(group2_codes), 6):
                        batch_codes = group2_codes[batch_num:batch_num + 6]
                        batch_display = ", ".join(batch_codes)
                        logger.info(f"æ­£åœ¨è™•ç†æœ‰æ©Ÿæœƒå™´ è§€å¯Ÿä¸€ä¸‹ç¬¬ {batch_num//6 + 1} çµ„: {batch_display}")

                        chart_path = plot_stock_charts(batch_codes, hist)
                        if chart_path:
                            img_url = upload_image(chart_path)
                            if img_url:
                                try:
                                    push_image(img_url, img_url)
                                    logger.info(f"âœ… åœ–è¡¨å·²ç™¼é€åˆ° LINE")
                                except Exception as e:
                                    logger.error(f"âŒ LINE ç™¼é€å¤±æ•—: {e}")
                                os.unlink(chart_path)
                            else:
                                logger.warning(f"âŒ åœ–åºŠä¸Šå‚³å¤±æ•—")
                        else:
                            logger.warning(f"âŒ åœ–è¡¨ç”Ÿæˆå¤±æ•—")

        # æ­¥é©Ÿ 8: åŒæ­¥è³‡æ–™åº«åˆ° Google Driveï¼ˆå¦‚æœæœ‰æ›´æ–°è³‡æ–™ï¼‰
        if data_updated and drive_service:
            logger.info("\nğŸ“Œ æ­¥é©Ÿ 8: åŒæ­¥è³‡æ–™åº«åˆ° Google Drive")
            sync_database_to_drive(drive_service)
        elif drive_service:
            logger.info("\næ­¥é©Ÿ 8: è³‡æ–™ç„¡æ›´æ–°ï¼Œè·³é Google Drive åŒæ­¥")
        else:
            logger.info("\næ­¥é©Ÿ 8: Google Drive æœå‹™ä¸å¯ç”¨ï¼Œè·³éåŒæ­¥")

        # ä»»å‹™å®Œæˆ
        end_time = datetime.now()
        execution_time = end_time - start_time
        logger.info("\n" + "=" * 50)
        logger.info(f"ğŸ‰ ä»»å‹™å®Œæˆï¼åŸ·è¡Œæ™‚é–“: {execution_time}")
        logger.info("=" * 50)

    except Exception as e:
        logger.error(f"âŒ ç¨‹å¼åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}")
        if DEBUG_MODE:
            logger.debug(f"è©³ç´°éŒ¯èª¤: {str(e)}", exc_info=True)
        raise
    finally:
        if DEBUG_MODE:
            logger.debug("ç¨‹å¼åŸ·è¡ŒçµæŸ")


if __name__ == "__main__":
    main()