"""
è‚¡ç¥¨æ•¸æ“šè™•ç†æ¨¡çµ„ - ä¸‹è¼‰è‚¡åƒ¹æ•¸æ“šå’Œé¸è‚¡é‚è¼¯
"""
from datetime import datetime, timedelta
import pandas as pd
import yfinance as yf
import time
from .database import get_existing_data_range
from .logger import get_logger

logger = get_logger(__name__)


def fetch_prices_yf(codes, lookback_days=120) -> pd.DataFrame:
    """
    å¾ Yahoo Finance ä¸‹è¼‰è‚¡åƒ¹æ•¸æ“š

    Args:
        codes: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
        lookback_days: å›æº¯å¤©æ•¸

    Returns:
        DataFrame: åŒ…å«è‚¡åƒ¹æ•¸æ“šçš„ DataFrame
    """
    existing = get_existing_data_range()
    target_start = (datetime.utcnow() - timedelta(days=lookback_days * 2)).date().isoformat()

    codes_to_fetch = []
    for c in codes:
        c = c.strip()
        if not c:
            continue
        if c not in existing:
            codes_to_fetch.append(c)
            logger.info(f"{c}: ç„¡æ­·å²è³‡æ–™ï¼Œéœ€ä¸‹è¼‰")
        else:
            max_date = existing[c]["max"]
            if max_date < datetime.utcnow().date().isoformat():
                codes_to_fetch.append(c)
                logger.info(f"{c}: è³‡æ–™éèˆŠ (æœ€æ–°: {max_date})ï¼Œéœ€æ›´æ–°")
            else:
                logger.debug(f"{c}: è³‡æ–™å·²æ˜¯æœ€æ–° (æœ€æ–°: {max_date})")

    if not codes_to_fetch:
        logger.info("æ‰€æœ‰è‚¡ç¥¨è³‡æ–™éƒ½å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€ä¸‹è¼‰")
        return pd.DataFrame()

    # ç‚ºäº†é¿å… Yahoo Finance API é™æµï¼Œæ¡ç”¨åˆ†æ‰¹ä¸‹è¼‰ç­–ç•¥
    # æ¯æ‰¹æœ€å¤š 200 æ”¯è‚¡ç¥¨ï¼Œæ‰¹æ¬¡ä¹‹é–“å»¶é² 3 ç§’
    BATCH_SIZE = 200
    BATCH_DELAY = 3  # ç§’

    logger.info(f"\né–‹å§‹ä¸‹è¼‰ {len(codes_to_fetch)} æ”¯è‚¡ç¥¨")
    logger.info(f"æœŸé–“: {target_start} ~ ä»Šæ—¥")

    # å¦‚æœè‚¡ç¥¨æ•¸é‡è¶…é BATCH_SIZEï¼Œæ¡ç”¨åˆ†æ‰¹ä¸‹è¼‰
    if len(codes_to_fetch) > BATCH_SIZE:
        num_batches = (len(codes_to_fetch) + BATCH_SIZE - 1) // BATCH_SIZE
        logger.info(f"âš ï¸  è‚¡ç¥¨æ•¸é‡è¼ƒå¤šï¼Œå°‡åˆ†æˆ {num_batches} æ‰¹ä¸‹è¼‰ï¼ˆæ¯æ‰¹ {BATCH_SIZE} æ”¯ï¼‰")
        logger.info(f"   æ‰¹æ¬¡ä¹‹é–“å»¶é² {BATCH_DELAY} ç§’ï¼Œä»¥é¿å… API é™æµ")

    all_results = []
    for batch_idx in range(0, len(codes_to_fetch), BATCH_SIZE):
        batch_codes = codes_to_fetch[batch_idx:batch_idx + BATCH_SIZE]
        batch_num = batch_idx // BATCH_SIZE + 1
        total_batches = (len(codes_to_fetch) + BATCH_SIZE - 1) // BATCH_SIZE

        if total_batches > 1:
            logger.info(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches}: ä¸‹è¼‰ {len(batch_codes)} æ”¯è‚¡ç¥¨")

        tickers = [f"{c}.TW" for c in batch_codes]

        try:
            df = yf.download(
                tickers=" ".join(tickers),
                start=target_start,
                interval="1d",
                group_by="ticker",
                auto_adjust=False,
                progress=False,
            )
            logger.info(f"   âœ… æ‰¹æ¬¡ {batch_num} ä¸‹è¼‰å®Œæˆï¼Œè³‡æ–™é¡å‹: {type(df)}, å½¢ç‹€: {df.shape if hasattr(df, 'shape') else 'N/A'}")

            # è™•ç†é€™æ‰¹è³‡æ–™
            if df is None or (isinstance(df, pd.DataFrame) and df.empty):
                logger.warning(f"   âš ï¸  æ‰¹æ¬¡ {batch_num} è¿”å›ç©ºè³‡æ–™")
            else:
                batch_results = []
                for c in batch_codes:
                    t = f"{c}.TW"
                    if isinstance(df, pd.DataFrame) and t in df:
                        tmp = df[t].reset_index().rename(columns=str.lower)
                        if "date" in tmp.columns:
                            tmp["date"] = pd.to_datetime(tmp["date"]).dt.tz_localize(None)
                        tmp["code"] = c
                        batch_results.append(tmp[["code", "date", "open", "high", "low", "close", "volume"]])
                    else:
                        logger.debug(f"   è‚¡ç¥¨ {c}: æ‰¹æ¬¡ä¸­ç„¡è³‡æ–™")

                if batch_results:
                    all_results.extend(batch_results)
                    logger.info(f"   âœ… æ‰¹æ¬¡ {batch_num} æˆåŠŸè™•ç† {len(batch_results)} æ”¯è‚¡ç¥¨")

        except Exception as e:
            logger.error(f"   âŒ æ‰¹æ¬¡ {batch_num} ä¸‹è¼‰å¤±æ•—: {e}")
            logger.error(f"   å¯èƒ½åŸå› ï¼šAPI é™æµæˆ–ç¶²è·¯å•é¡Œ")
            # ç¹¼çºŒè™•ç†ä¸‹ä¸€æ‰¹ï¼Œä¸ä¸­æ–·æ•´å€‹æµç¨‹

        # å¦‚æœé‚„æœ‰ä¸‹ä¸€æ‰¹ï¼Œå»¶é²ä¸€æ®µæ™‚é–“é¿å…é™æµ
        if batch_idx + BATCH_SIZE < len(codes_to_fetch):
            logger.debug(f"   â¸ï¸  å»¶é² {BATCH_DELAY} ç§’å¾Œç¹¼çºŒä¸‹ä¸€æ‰¹...")
            time.sleep(BATCH_DELAY)

    # åˆä½µæ‰€æœ‰æ‰¹æ¬¡çš„çµæœ
    if not all_results:
        logger.error("âŒ æ‰€æœ‰æ‰¹æ¬¡éƒ½æœªèƒ½æˆåŠŸä¸‹è¼‰è³‡æ–™")
        logger.error("   å¯èƒ½åŸå› ï¼š")
        logger.error("   1. Yahoo Finance API æš«æ™‚ç„¡æ³•è¨ªå•")
        logger.error("   2. ç¶²è·¯é€£ç·šå•é¡Œ")
        logger.error("   3. API é™æµï¼ˆToo Many Requestsï¼‰")
        logger.error("   å»ºè­°ï¼šç¨å¾Œé‡è©¦æˆ–æª¢æŸ¥ GitHub Actions æ—¥èªŒ")
        return pd.DataFrame()

    # åˆä½µæ‰€æœ‰æ‰¹æ¬¡çš„çµæœ
    result = pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()
    logger.info(f"æˆåŠŸä¸‹è¼‰ {len(result)} ç­†æ•¸æ“š")
    if not result.empty and 'date' in result.columns:
        logger.info(f"åˆä½µå¾Œç¸½æ—¥æœŸç¯„åœ: {result['date'].min()} ~ {result['date'].max()}")
        logger.info(f"åˆä½µå¾Œå”¯ä¸€æ—¥æœŸæ•¸: {result['date'].nunique()}")
    return result


def pick_stocks(prices: pd.DataFrame, top_k=30) -> pd.DataFrame:
    """
    å‹•èƒ½é¸è‚¡ç­–ç•¥ - é¸å‡ºç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨

    ç­–ç•¥èªªæ˜:
    1. æ’é™¤è¿‘10æ—¥å¹³å‡æˆäº¤é‡å°æ–¼1000å¼µçš„è‚¡ç¥¨
    2. è¨ˆç®— MA20 (20æ—¥ç§»å‹•å¹³å‡ç·š)
    3. æª¢æŸ¥æœ€è¿‘5å¤©é–‹ç›¤åƒ¹èˆ‡æ”¶ç›¤åƒ¹çš„å¹³å‡éƒ½åœ¨MA20ä¹‹ä¸Š
    4. MA20 æ–œç‡åœ¨åˆç†ç¯„åœå…§ (< 1)
    5. æ³¢å‹•ç‡æ§åˆ¶åœ¨ 5% ä»¥å…§
    6. è¿‘åæ—¥çš„æœ€é«˜é»æ¸›æœ€ä½é»çš„å¹³å‡è¦å¤§æ–¼1å¡Š
    7. åƒ¹æ ¼èˆ‡ MA20 è·é›¢åœ¨å…è¨±ç¯„åœå…§
    8. ä¾ç…§ MA20 æ–œç‡åˆ†çµ„ï¼Œé¸å‡ºæœ€æ¥è¿‘ MA20 çš„è‚¡ç¥¨

    Args:
        prices: è‚¡åƒ¹æ•¸æ“š DataFrame
        top_k: æœ€å¤šè¿”å›å¹¾æ”¯è‚¡ç¥¨ (æœªä½¿ç”¨ï¼Œä¿ç•™åƒæ•¸å‘å¾Œç›¸å®¹)

    Returns:
        DataFrame: é¸è‚¡çµæœ
    """
    if prices.empty or 'code' not in prices.columns:
        logger.warning("è‚¡åƒ¹è³‡æ–™ç‚ºç©ºæˆ–ç¼ºå°‘å¿…è¦æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œé¸è‚¡")
        return pd.DataFrame()
    prices = prices.sort_values(["code", "date"])

    # ä½¿ç”¨ transform ä»£æ›¿ apply ä»¥ç¢ºä¿ä¿ç•™æ‰€æœ‰æ¬„ä½ï¼ˆåŒ…æ‹¬ codeï¼‰
    # é€™é¿å…äº†æŸäº› pandas ç‰ˆæœ¬ä¸­ group_keys=False å°è‡´ code æ¬„ä½æ¶ˆå¤±çš„å•é¡Œ
    prices = prices.copy()
    prices["ma20"] = prices.groupby("code")["close"].transform(lambda x: x.rolling(20, min_periods=20).mean())
    feat = prices

    results = []
    for code, group in feat.groupby("code"):
        group = group.sort_values("date")
        if len(group) < 10:
            continue

        # æª¢æŸ¥è¿‘10æ—¥å¹³å‡æˆäº¤é‡æ˜¯å¦å¤§æ–¼1000å¼µï¼ˆ1å¼µ=1000è‚¡ï¼‰
        last_10 = group.tail(10)
        avg_volume = last_10["volume"].mean()
        avg_volume_lots = avg_volume / 1000  # è½‰æ›ç‚ºå¼µæ•¸

        if avg_volume_lots < 1000:
            continue

        last_5 = group.tail(5)
        if last_5["ma20"].isna().any():
            continue

        # æª¢æŸ¥æœ€è¿‘5å¤©é–‹ç›¤åƒ¹èˆ‡æ”¶ç›¤åƒ¹çš„å¹³å‡éƒ½åœ¨MA20ä¹‹ä¸Š
        avg_price_5d = (last_5["open"] + last_5["close"]) / 2
        price_above_ma20 = (avg_price_5d > last_5["ma20"]).all()

        if not price_above_ma20:
            continue

        # è¿‘åæ—¥çš„æœ€é«˜é»æ¸›æœ€ä½é»çš„å¹³å‡è¦å¤§æ–¼1å¡Š
        high_low_diff = last_10["high"] - last_10["low"]
        avg_high_low_diff = high_low_diff.mean()

        if avg_high_low_diff <= 1.0:
            continue

        # è¨ˆç®— MA20 æ–œç‡
        ma20_values = last_5["ma20"].values
        ma20_slope = (ma20_values[-1] - ma20_values[0]) / 4

        # éæ¿¾æ‰æ–œç‡éå¤§çš„è‚¡ç¥¨
        if ma20_slope >= 1:
            continue

        # è¨ˆç®—æ³¢å‹•ç‡
        price_std = last_5["close"].std()
        price_mean = last_5["close"].mean()
        volatility_pct = (price_std / price_mean * 100) if price_mean > 0 else 999
        if volatility_pct > 5.0:
            continue

        # å‹•æ…‹èª¿æ•´è·é›¢é™åˆ¶
        max_distance_allowed = max(2.0, volatility_pct * 1.5)

        # è¨ˆç®—åƒ¹æ ¼èˆ‡ MA20 çš„è·é›¢
        min_price = last_5[["open", "close"]].min(axis=1)
        distance_pct = ((min_price - last_5["ma20"]) / last_5["ma20"] * 100)
        avg_distance = distance_pct.mean()

        if avg_distance > max_distance_allowed:
            continue

        # è¨ˆç®—å¹³å‡è·é›¢ MA20
        avg_price = (last_5["open"] + last_5["close"]) / 2
        avg_ma20_distance = abs(avg_price - last_5["ma20"]).mean()

        # åˆ¤æ–·æœ€å¾Œä¸€å¤©æ˜¯å¦ç‚ºæœ€ä½æ”¶ç›¤åƒ¹
        latest = last_5.iloc[-1]
        is_lowest_close = latest["close"] == last_5["close"].min()

        results.append({
            "code": code,
            "close": latest["close"],
            "ma20": latest["ma20"],
            "distance": avg_distance,
            "volatility": volatility_pct,
            "ma20_slope": ma20_slope,
            "max_distance": max_distance_allowed,
            "volume": latest["volume"],
            "avg_volume_10d": avg_volume,
            "avg_volume_10d_lots": avg_volume_lots,
            "avg_ma20_distance": avg_ma20_distance,
            "is_lowest_close": is_lowest_close
        })

    if not results:
        return pd.DataFrame()

    result_df = pd.DataFrame(results)

    # ä¾ç…§ MA20 æ–œç‡åˆ†çµ„
    group1 = result_df[(result_df["ma20_slope"] >= 0.5) & (result_df["ma20_slope"] < 1)]
    group2 = result_df[result_df["ma20_slope"] < 0.5]

    # Group1: MA20 æ–œç‡ >= 0.5ï¼Œæœ€å¤šé¸ 6 æ”¯
    if len(group1) > 6:
        group1_filtered = group1[group1["is_lowest_close"] == False]
        if len(group1_filtered) > 6:
            group1 = group1_filtered.nsmallest(6, "avg_ma20_distance")
        elif len(group1_filtered) > 0:
            group1 = group1_filtered
        else:
            group1 = group1.nsmallest(6, "avg_ma20_distance")

    # Group2: MA20 æ–œç‡ < 0.5ï¼Œæœ€å¤šé¸ 6 æ”¯
    if len(group2) > 6:
        group2_filtered = group2[group2["is_lowest_close"] == False]
        if len(group2_filtered) > 6:
            group2 = group2_filtered.nsmallest(6, "avg_ma20_distance")
        elif len(group2_filtered) > 0:
            group2 = group2_filtered
        else:
            group2 = group2.nsmallest(6, "avg_ma20_distance")

    final_result = pd.concat([group1, group2], ignore_index=True)
    return final_result